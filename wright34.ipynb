{"cells":[{"cell_type":"markdown","metadata":{},"source":"Notes on Various Topics for ARE212\n==================================\n\n**Author:** Ethan Ligon\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Kernel Density Estimation:PROPERTIES:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The Wiggly Distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We address the following problem.  There exists a random variable\nwith a density $f$.  We call this distribution the &ldquo;wiggly&rdquo;\ndistribution, and use the magic of `scipy.stats` to define it as follows:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import rv_continuous\nimport numpy as np\n\nclass wiggly_gen(rv_continuous):\n\n    \"\"\"Wiggly distribution\"\"\"\n\n    def _pdf(self, x):\n        d = self.b - self.a\n        return (np.sin(x*6*2*np.pi/d) + 1)/(self.b - self.a)\n\nwiggly = wiggly_gen(a=0.,b=2*np.pi,name='wiggly')\n\nx = wiggly()\n\nf = x.pdf  # Name of true pdf, for pedagogical convenience"]},{"cell_type":"markdown","metadata":{},"source":["Plot the pdf:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_pdf(x,npts=1000,ax=None):\n\n    if ax is None:\n        fig,ax = plt.subplots()\n\n    V = np.linspace(x.a,x.b,npts)\n    ax.plot(V,[x.pdf(v) for v in V])\n\n    return ax\n\nax = plot_pdf(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Sample\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, suppose we don&rsquo;t know the distribution of the random variable\n$x$, but we can draw a sample of realizations, which we&rsquo;ll put in a `pandas.Series`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n\nn = 1000 # Sample size\n\nS = pd.Series(x.rvs(size=n))"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["How can we use this random sample to estimate the density?\nOne simple idea is to look at the histogram (scaled to sum to one):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["S.hist(bins=int(np.sqrt(len(S))),density=True)"]},{"cell_type":"markdown","metadata":{},"source":["This captures some of the important features of the distribution, but\nlet&rsquo;s see if we can do better by using kernel density tools.\n\nStart by defining a kernel:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["sqrt3 = np.sqrt(3)  # Avoid repeated evaluation of this for speed...\n\nk = lambda u: (np.abs(u) < sqrt3)/(2*sqrt3)  # Rectangular kernel\n\n# k = lambda u: np.exp(-(u**2)/2)/np.sqrt(2*np.pi) # Gaussian kernel"]},{"cell_type":"markdown","metadata":{},"source":["Now define the kernel estimator:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def kernel_estimator(X,h):\n    \"\"\"\n    Use data X to estimate a density, using bandwidth h.\n    \"\"\"\n    return lambda x: k((X-x)/h).mean()/h"]},{"cell_type":"markdown","metadata":{},"source":["We already have a random sample: let&rsquo;s try using it to estimate $f$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fhat = kernel_estimator(S,0.05) \n\nfhat(1)  # Try to evaluate at a point"]},{"cell_type":"markdown","metadata":{},"source":["Now graph our estimate\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x) # \"True\" distribution\n\nV = np.linspace(0,2*np.pi,1000)\n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["Play with other bandwidths, and compare with &ldquo;truth&rdquo;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x) # \"True\" distribution\n\nfhat = kernel_estimator(S,0.25) \n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["Consider the Silverman rule of thumb:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["h_silverman = S.std()*S.count()**(-1/5)*1.06\n\nax = plot_pdf(x) # \"True\" distribution\n\nfhat = kernel_estimator(S,h_silverman) \n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["### Bias\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Hansen (Probability, &sect; 17.4) develops an expression for the bias of\nthe  kernel density estimator:\n$$\n  \\mbox{Bias}(x) = \\int k(u)\\left(f(x+hu)-f(x)\\right)du.\n$$\nThus, bias depends only on the kernel, the bandwidth, and the\n(unknown!) density $f$.\n\nHere we use an &ldquo;oracle&rdquo; estimator to estimate the bias, in which we\nmake use of the fact that (for the experiment in this notebook) we in\nfact *do* do know $f$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Oracle Bias Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.integrate import quad # General purpose integration\n\nintegrand = lambda u,x,h: k(u)*(f(x + h*u) - f(x))\n\n# bias will return a pair (integral,absolute error tolerance),\n# where the latter is a promise that the absolute error \n# from numerical integration is smaller than the reported tolerance.\nbias = lambda x,h: quad(lambda u: integrand(u,x,h), # Hold x & h fixed in integral\n                        a=0, b=2*np.pi)   # Limits of integration"]},{"cell_type":"markdown","metadata":{},"source":["Try evaluating this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["bias(x=1,h=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["Try plotting!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x)\nh = 0.1\nax.plot(V,[bias(v,h=h) for v in V])"]},{"cell_type":"markdown","metadata":{},"source":["Compare with actual estimate:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x)\n\nh = 0.1\n\nfhat = kernel_estimator(S,h=h) \n\nax.plot(V,[bias(v,h=h) for v in V])\n\nax.plot(V,[fhat(v) for v in V])"]},{"cell_type":"markdown","metadata":{},"source":["### Variance\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The finite-sample *variance* of the kernel estimator (so now sampling\nvariation matters) can be computed as just the variance of the sample\n&ldquo;smooths&rdquo; $k(x_i - x)/h$, so\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def Vhat(X,h):\n    \"\"\"\n    Use data X to estimate the variance of $\\hat{f}$ as a function of $x$.\n    \"\"\"\n    n = X.count()\n    return lambda x: k((X-x)/h).var()/(n*h**2)"]},{"cell_type":"markdown","metadata":{},"source":["Try evaluating this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["vhat = Vhat(S,h=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["Try plotting!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["plt.plot(V,[vhat(v) for v in V])"]},{"cell_type":"markdown","metadata":{},"source":["## Kernel Regression:PROPERTIES:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Conditional expectations\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We address the following problem.  There exists a random variable $y$\n  with conditional expectation $E(y|x)$  which is continuous in\n  $x$ (further, if $x$ is random then the density of $x$, $f(x)>0$).\n  For example (play!):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n\ndef m(x): return np.sin(2*x)/(1+x)"]},{"cell_type":"markdown","metadata":{},"source":["Plot the conditional expectation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\nimport matplotlib.pyplot as plt\n\nV = np.linspace(1e-4,2*np.pi,1000)\n\nax = plt.plot(V,m(V))"]},{"cell_type":"markdown","metadata":{},"source":["### Sample\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, suppose we don&rsquo;t know $m$ or the conditional distribution of $y$,\nbut we can draw a sample of realizations, which we&rsquo;ll put in a `pandas.Series`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\nfrom scipy.stats import distributions as iid\n\nX = iid.uniform(V.min(),V.max())  # Handy to have bounded support, but not necessary\n\nU = iid.norm(scale=0.1) # Scale=std. dev.  Play!\n\nn = 1000 # Sample size\n\nx = pd.Series(X.rvs(size=n))\n\n# Series are imperial when they can be:\ny = m(x) + U.rvs(size=n)  \n\ntype(y)"]},{"cell_type":"markdown","metadata":{},"source":["Now let&rsquo;s take an informal look at the data:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["plt.scatter(x,y)"]},{"cell_type":"markdown","metadata":{},"source":["How can we use this random sample to estimate $m$?  The scatterplot\nsuggests that some kind of local smoothing might help.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let&rsquo;s go ahead and define the kernel regression estimator, following\nthe same general logic as in our exploration of kernel densities.\n\nStart by defining a kernel:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["sqrt3 = np.sqrt(3)  # Avoid repeated evaluation of this for speed...\n\nk = lambda u: (np.abs(u) < sqrt3)/(2*sqrt3)  # Rectangular kernel\n\n# k = lambda u: np.exp(-(u**2)/2)/np.sqrt(2*np.pi) # Gaussian kernel"]},{"cell_type":"markdown","metadata":{},"source":["Now define the kernel estimator:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def kernel_regression(X,y,h):\n    \"\"\"\n    Use data (X,y) to estimate E(y|x), using bandwidth h.\n    \"\"\"\n    def mhat(x):\n        S = k((X-x)/h) # \"Smooths\"\n\n        return S.dot(y)/S.sum()\n\n    return mhat"]},{"cell_type":"markdown","metadata":{},"source":["We already have a random sample: let&rsquo;s try using it to estimate $m$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mhat = kernel_regression(x,y,h=0.3) \n\nmhat(1)  # Try to evaluate at a point"]},{"cell_type":"markdown","metadata":{},"source":["Now graph our estimate\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fig,ax = plt.subplots()\n\nax.plot(V,[m(v) for v in V])  # \"Truth\"\nax.plot(V,[mhat(v) for v in V]) # Estimate"]},{"cell_type":"markdown","metadata":{},"source":["### Leave-one out cross-validation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We talked about computing estimates $m_{-i}$ to estimate the IMSE.\n   Doing this directly would require $n$ separate estimations.  This\n   is expensive!\n\nA little algebra suggests an alternative, involving the &ldquo;Gram&rdquo; matrix.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def leave_out_residuals(X,y):\n    \"\"\"\n    Use data (X_{-j},y_{-j}) to estimate E(y|x), using bandwidth h.\n    \"\"\"\n    \n    def Gram_matrix(X,h):\n        \"\"\"\n        Define kernel-data matrix [ k((x_i-x_j)/h) ]\n        \"\"\"\n        v = np.array(X).reshape((-1,1)) # Column vector\n\n        return k((v-v.T)/h) \n\n    def ecv(h):\n        \"\"\"\n        Leave-out residuals as fn of bandwidth h.\n        \"\"\"\n        G = Gram_matrix(X,h)\n        G0 = G - np.diag(np.diag(G)) # Zero out diagonals\n\n        e = y - (G0@y)/np.sum(G0,axis=1)\n\n        return e\n\n    return ecv"]},{"cell_type":"markdown","metadata":{},"source":["Consider the &ldquo;leave-one-out&rdquo; or &ldquo;Cross-Validation&rdquo; residuals:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ecv = leave_out_residuals(x,y)\n\nax = ecv(h=0.1).hist()"]},{"cell_type":"markdown","metadata":{},"source":["Define an estimator of the IMSE as a function of the bandwidth $h$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def imse_cv(h):\n    return np.mean(ecv(h)**2)\n\nimse_cv(.5)"]},{"cell_type":"markdown","metadata":{},"source":["Try plotting esimated IMSE as a function of bandwidth:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["H = np.linspace(1e-3,.5,20)\n\nplt.plot(H,[imse_cv(h) for h in H])"]},{"cell_type":"markdown","metadata":{},"source":["Actually find the minimum:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize_scalar\n\nsoltn = minimize_scalar(imse_cv,bounds=[1e-2,1])\n\nif soltn.success:\n    hstar = soltn.x\n    print(soltn)"]},{"cell_type":"markdown","metadata":{},"source":["Now, estimate using estimated optimal bandwidth:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mhat = kernel_regression(x,y,hstar)\n\nfig,ax = plt.subplots()\n\nax.plot(V,[m(v) for v in V])  # \"Truth\"\nax.plot(V,[mhat(v) for v in V]) # Estimate"]},{"cell_type":"markdown","metadata":{},"source":["## Quantities & Prices (Wright 1934)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider a data generating process for $(q,p)$ based\non Goldberger (1972), who in turn is describing the work of Sewall\nWright (1934).  The demand-supply system is\n$$\n   q_D = \\alpha p + u\\qquad q_S = \\beta p + v\\qquad q_D = q_S,\n$$\nwhere $(u,v)$ are unobserved shocks to demand and supply,\nrespectively.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nfrom scipy.stats import distributions as iid\n\n# Structural parameters;\n(α,β) = (-1,2)     \nσ = {'u':1/2,'v':1/3}\nμ = {'u':2,'v':-1}\n\n# u,v assumed independent\nu = iid.norm(loc=μ['u'], scale=σ['u'])  # Demand shocks\nv = iid.norm(loc=μ['v'], scale=σ['v'])  # Supply shocks\n\n# Reduced form coefficients\nπ = [[-β/(α - β), -1/(α - β)],\n     [ α/(α - β), 1/(α - β)]]\n\n# Generate N realizations of system\n# Outcomes Y have columns (q,p)\nN = 10\n\n# Arrange shocks into an Nx2 matrix\nU = np.c_[u.rvs(N), v.rvs(N)]\n\n# Matrix product gives [q,p]; label by putting into df\ndf = pd.DataFrame(U@π,columns=['q','p'])\nUdf = pd.DataFrame(U,columns=['u','v']) # For future reference"]},{"cell_type":"markdown","metadata":{},"source":["We can interrogate these data:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["And compute the linear correlation&#x2026;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df.corr()"]},{"cell_type":"markdown","metadata":{},"source":["Or more generally the covariance matrix:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["C=df.cov()\nC"]},{"cell_type":"markdown","metadata":{},"source":["From which we can calculate the linear regression coefficient\nof $p = a + bq + e$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["C.loc['p','q']/C.loc['q','q']"]},{"cell_type":"markdown","metadata":{},"source":["And learn about the probability density&#x2026;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy import stats\nimport numpy as np\n\n# Estimate joint density of (q,p)\npdf = stats.gaussian_kde(df.T).pdf \n\nax = df.plot.scatter(x='q',y='p')\n\nv = ax.axis()\nQ = np.mgrid[v[0]:v[1]:100j].tolist()\nP = np.mgrid[v[2]:v[3]:100j].tolist()\n\n_ = ax.contour(Q,P,np.array([[pdf((q,p))[0] for p in P] for q in Q]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Counterfactual Demand & Supply Schedules\n\n"]},{"cell_type":"markdown","metadata":{},"source":["What are the actual *counterfactual* demand and\nsupply schedules?  This is the kind of thing that Frisch described as\n&ldquo;hypothetical experiments.&rdquo;  The schedules respond to shocks $u$ and $v$, respectively,\nyielding\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["qmax = df['q'].max()\nqmin = df['q'].min()\n\nQ = pd.DataFrame({'min':np.maximum(0,df['q']-0.3*(qmax-qmin)),\n                  'max':np.minimum(qmax*1.2,df['q']+0.3*(qmax-qmin)),\n                  'miss':-1})\n\n# Inverse counterfactual demand & supply (for plotting)\nD = Q.add(-Udf['u'],axis=0)/α  \nS = Q.add(-Udf['v'],axis=0)/β\n\ncounterfactual=pd.DataFrame({'S':S.stack(),\n                             'D':D.stack(),\n                             'Q':Q.stack()})\n\ncounterfactual=counterfactual.replace(-1,np.nan)\n\n_ = counterfactual.plot(x='Q')"]},{"cell_type":"markdown","metadata":{},"source":["#### Controlling Price\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the question: what would expected demand be if we *fixed*\n    the price at $p_0$?  Expected supply?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Average Causal Effect of a Change in Price\n\n"]},{"cell_type":"markdown","metadata":{},"source":["What would expected demand be if we *observed* that the price was $p_0$?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Price Change *Ceteris Paribus*\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Suppose we *observe* prices and quantities $(p_0,q_0)$.  How *would*\n    we expect the quantity demanded to change if prices were instead\n    fixed at $p_1$, *ceteris paribus*?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Instrumental Variables in Canonical Demand & Supply Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data-Generating Process\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nfrom scipy.stats import distributions as iid\n\n# Unobservable component of supply shock z\n# Can have any distribution one pleases\nw = iid.beta(1,2,loc=-iid.beta(1,2).mean()) # Centered for convenience\n\n# Structural parameters;\n(alpha,beta) = (-1,2)     \nsigma = {'u':1/2,'v':1/3}\nmu = {'u':2,'v':-1}\n\n# u,v assumed independent\nu = iid.norm(loc=mu['u'], scale=sigma['u'])  # Demand shocks\nv = iid.norm(loc=mu['v'], scale=sigma['v'])  # Supply shocks\n\n# Reduced form coefficients\npi = [[-beta/(alpha - beta), -1/(alpha - beta)],\n     [ alpha/(alpha - beta), 1/(alpha - beta)]]\n\n# Generate N realizations of system\n# Outcomes have columns (p,q,z)\ndef wright_dgp(N):\n    \"\"\"\n    Generate data consistent with Wright (1934) hog demand and supply.\n\n    Returns a pandas dataframe with N observations on (p,q,z), where\n    z is understood to be a supply shock.\n    \"\"\"\n    \n    # Arrange shocks into an Nx2 matrix\n    U = np.c_[u.rvs(N), v.rvs(N)]\n\n    # Matrix product gives [q,p]; label by putting into df\n    df = pd.DataFrame(U@pi,columns=['q','p'])\n\n    Udf = pd.DataFrame(U,columns=['u','v']) # For future reference\n\n    # Relate v and z (need not be linear)\n    unobserved_shock = w.rvs(N)/10\n    df['z'] = (1-unobserved_shock)*np.exp(4*Udf['v'] - unobserved_shock)\n    df['Constant'] = 1\n\n    # Include a constant term in both X & Z\n    return df[['q']],df[['Constant','p']],df[['Constant','z']]"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let&rsquo;s write some code to estimate the parameters of the regression\n   model using the estimator devised above (the &ldquo;simple IV estimator&rdquo;):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n\ndef draw_b(N,dgp):\n    \"\"\"\n    Generate a random variate $b$ from a sample of $N$ draws from the Wright (1934) DGP.\n    \"\"\"\n    y,X,Z =  dgp(N)\n\n    return np.linalg.solve(Z.T@X,Z.T@y) # Solve normal eqs\n\nb = draw_b(10000,wright_dgp)\n\nprint(b)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now consider the point that the estimator $b$ is a random variable.\n Under the assumptions of the *model* a Central Limit Theorem applies,\n so it&rsquo;s asymptotically normal.  But in any finite sample the just\n identified linear IV estimator can be feisty.  Let&rsquo;s explore using a\n little Monte Carlo experiment.  Let&rsquo;s begin by constructing a\n slightly more transparent data-generating process, in which $Z$ and\n $X$ have a linear relationship:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef linear_dgp(N,beta,gamma,pi,sigma_u,sigma_v):\n    u = iid.norm(scale=sigma_u).rvs(N)\n    v = iid.norm(scale=sigma_v).rvs(N)\n    Z = iid.norm().rvs(N)\n\n    X = Z*pi + v\n    y = X*beta + u\n\n    df = pd.DataFrame({'y':y,'x':X,'z':Z,'Constant':1})\n\n    return df[['y']],df[['Constant','x']],df[['Constant','z']]"]},{"cell_type":"markdown","metadata":{},"source":["The next bit of code *repeatedly* draws new random samples and\n  calculates $b$ from them; we then construct a histogram of the\n  resulting estimates.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n\nB = pd.DataFrame([draw_b(100,lambda N: linear_dgp(N,1,0,.01,1,1))[1] for i in range(1000)])\nB.hist(bins=int(np.ceil(np.sqrt(B.shape[0]))))"]},{"cell_type":"markdown","metadata":{},"source":["## Finite Sample Properties of Linear GMM\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["GMM provides a generalized way to think about instrumental\n   variables estimators, and we also have evidence that the finite\n   sample properties of these estimators may be poor.  Here we&rsquo;ll\n   construct a simple Monte Carlo framework within which to evaluate\n   the finite-sample behavior of GMM linear IV estimators.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Asymptotic Variance of GMM estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["If we have $\\mbox{E}g_j(\\beta)g_j(\\beta)^\\top=\\Omega$ and\n   $\\mbox{E}\\frac{\\partial g_j}{\\partial b^\\top}(\\beta)=Q$ then we&rsquo;ve\n   seen that the asymptotic variance of the optimally weighted GMM\n   estimator is\n   $$\n       V_b = \\left(Q^\\top\\Omega^{-1}Q\\right)^{-1}.\n   $$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generating Process\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We consider the linear IV model\n\n\\begin{align*}\n   y &= X\\beta + u\\\\\n   \\mbox{E}Z^\\top u &= 0\n\\end{align*}\n\nThus, we need to describe processes that generate $(X,Z,u)$.\n\nThe following code block defines the important parameters governing\nthe DGP; this is the &ldquo;TRUTH&rdquo; we&rsquo;re designing tools to reveal.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\n## Play with us!\nbeta = 1     # \"Coefficient of interest\"\ngamma = 1    # Governs effect of u on X\nsigma_u = 1  # Note assumption of homoskedasticity\n## Play with us!\n\n# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n\nell = 4 # Play with me too!\n\n# Arbitrary (but deterministic) choice for VXZ = [VX Cov(X,Z);\n#                                                 Cov(Z,X) VZ]\n# Pinned down by choice of a matrix A...\nA = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n\n## Below here we're less playful.\n\n# Now Var([X,Z]|u) is constructed so guaranteed pos. def.\nVXZ = A.T@A \n\nQ = -VXZ[1:,[0]]  # -EZX', or generally Edgj/db'\n\n# Gimme some truth:\ntruth = (beta,gamma,sigma_u,VXZ)\n\n## But play with Omega if you want to introduce heteroskedascity\nOmega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n\n# Asymptotic variance of optimally weighted GMM estimator:\nprint(inv(Q.T@inv(Omega)@Q))"]},{"cell_type":"markdown","metadata":{},"source":["Now code to generate $N$ realizations of $(y,X,Z)$ given some &ldquo;truth&rdquo;\n`(beta,gamma,sigma_u,VXZ)`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef dgp(N,beta,gamma,sigma_u,VXZ):\n    \"\"\"Generate a tuple of (y,X,Z).\n\n    Satisfies model:\n        y = X@beta + u\n        E Z'u = 0\n        Var(u) = sigma^2\n        Cov(X,u) = gamma*sigma_u^2\n        Var([X,Z}|u) = VXZ\n        u,X,Z mean zero, Gaussian\n\n    Each element of the tuple is an array of N observations.\n\n    Inputs include\n    - beta :: the coefficient of interest\n    - gamma :: linear effect of disturbance on X\n    - sigma_u :: Variance of disturbance\n    - VXZ :: Cov([X,Z|u])\n    \"\"\"\n    \n    u = iid.norm.rvs(size=(N,1))*sigma_u\n\n    # \"Square root\" of VXZ via eigendecomposition\n    lbda,v = np.linalg.eig(VXZ)\n    SXZ = v@np.diag(np.sqrt(lbda))\n\n    # Generate normal random variates [X*,Z]\n    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n\n    # But X is endogenous...\n    X = XZ[:,[0]] + gamma*u\n    Z = XZ[:,1:]\n\n    # Calculate y\n    y = X*beta + u\n\n    return y,X,Z"]},{"cell_type":"markdown","metadata":{},"source":["Check on DGP:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000\n\ndata = dgp(N,*truth)\n\ny,X,Z = data # Unpack tuple to check on things\n\n# Check that we've computed things correctly:\nprint(\"True Var([X,Z]|u):\")\nprint(VXZ,end=\"\\n\\n\")\n\nprint(\"Estimated (unconditional) sample covariance matrix minus true conditional:\")\nprint(np.cov(np.c_[X,Z].T,ddof=0) - VXZ)"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have a data-generating process we proceed under\n   the conceit that we can observe samples generated by this process,\n   but otherwise temporarily &ldquo;forget&rdquo; the properties of the DGP, and use the\n   generated data to try to reconstruct aspects of the DGP.\n\nIn our example, we consider using the optimally weighted linear IV\nestimator, and define a function which computes observation-level\ndeviations from expectations for this model. To estimate a different\nmodel this is the function we&rsquo;d want to re-define `gj`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\ndef gj(b,y,X,Z):\n    \"\"\"Observations of g_j(b).\n\n    This defines the deviations from the predictions of our model; i.e.,\n    e_j = Z_ju_j, where EZ_ju_j=0.\n\n    Can replace this function to testimate a different model.\n    \"\"\"\n    return Z*(y - X*b)"]},{"cell_type":"markdown","metadata":{},"source":["#### Construct sample moments\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Begin by defining a function to construct the sample moments given\n    the data and a parameter estimate $b$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def gN(b,data):\n    \"\"\"Averages of g_j(b).\n\n    This is generic for data, to be passed to gj.\n    \"\"\"\n    e = gj(b,*data)\n\n    # Check to see more obs. than moments.\n    assert e.shape[0] > e.shape[1]\n    \n    return e.mean(axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["#### Define estimator of Egg&rsquo;\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we define a function to compute covariance matrix of moments.\nRe-centering can be important in finite samples, even if irrelevant in\nthe limit.  Since we have $\\mbox{E}g_j(\\beta)=0$ under the null we may\nas well use this information when constructing our weighting matrix.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def Omegahat(b,data):\n    e = gj(b,*data)\n\n    # Recenter! We have Eu=0 under null.\n    # Important to use this information.\n    e = e - e.mean(axis=0) \n    \n    return e.T@e/e.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["Check construction of weighting matrix for our data at true parameter\n$\\beta$.  Note that we&rsquo;re &ldquo;cheating&rdquo; here, since our construction of\n`Winv` relied on knowledge of the true parameter $\\beta$ (this is\nsometimes called an &ldquo;Oracle estimator&rdquo;).  We&rsquo;ll play fair a bit later.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Winv = Omegahat(beta,data) \nprint(Winv)"]},{"cell_type":"markdown","metadata":{},"source":["Define the criterion function given a weighting matrix $W$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def J(b,W,data):\n\n    m = gN(b,data) # Sample moments @ b\n    N = data[0].shape[0]\n\n    return N*m.T@W@m # Scale by sample size"]},{"cell_type":"markdown","metadata":{},"source":["Next, check construction of criterion given our data.  We want\nsomething that looks nice and quadratic, at least in the neighborhood\nof $\\beta$.  Note that comparing the criterion to the critical values\nof the $\\chi^2$ statistic gives us an alternative way to construct\nconfidence intervals.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n%matplotlib inline\n\n# Limiting distribution of criterion (under null)\nlimiting_J = iid.chi2(ell-1)\n\n# Limiting SE of b\nsigma_0 = lambda N: np.sqrt(inv(Q.T@inv(Winv)@Q)/N)[0][0] \n\n# Choose 8 sigma_0 neighborhood of \"truth\"\nB = np.linspace(beta-4*sigma_0(N),beta+4*sigma_0(N),100)\nW = inv(Winv)\n\n_ = plt.plot(B,[J(b,W,data) for b in B.tolist()])\n_ = plt.axhline(limiting_J.isf(0.05),color='r')"]},{"cell_type":"markdown","metadata":{},"source":["#### Two Step Estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We next implement the two-step GMM estimator.  If we needed to\n    estimate more than a single scalar parameter we&rsquo;d need a different\n    minimization routine.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize_scalar\n\ndef two_step_gmm(data):\n\n    # First step uses identity weighting matrix\n    W1 = np.eye(gj(1,*data).shape[1])\n\n    b1 = minimize_scalar(lambda b: J(b,W1,data)).x \n\n    # Construct 2nd step weighting matrix using\n    # first step estimate of beta\n    W2 = inv(Omegahat(b1,data))\n\n    return minimize_scalar(lambda b: J(b,W2,data))"]},{"cell_type":"markdown","metadata":{},"source":["Now let&rsquo;s try it with an actual sample, just to see that things work:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["soltn = two_step_gmm(data)\n\nprint(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"]},{"cell_type":"markdown","metadata":{},"source":["### Monte Carlo Experiment\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now our experiment begins.  We set our frequentist hats firmly on our\nheads, and draw repeated samples of data, each generating a\ncorresponding estimate of beta.  Then the empirical distribution of\nthese samples tells us about the *finite* sample performance of our estimator.\n\nWe&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\nsamples of size $N$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000 # Sample size\n\nD = 1000 # Monte Carlo draws\n\nb_draws = []\nJ_draws = []\nfor d in range(D):\n    soltn = two_step_gmm(dgp(N,*truth))\n    b_draws.append(soltn.x)\n    J_draws.append(soltn.fun)\n\n_ = plt.hist(b_draws,bins=int(np.ceil(np.sqrt(N))))\n_ = plt.axvline(beta,color='r')"]},{"cell_type":"markdown","metadata":{},"source":["### Distribution of Monte Carlo draws vs. Asymptotic distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Compare Monte Carlo standard errors with asymptotic approximation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Limiting distribution of estimator\n\nlimiting_b = iid.norm(scale=sigma_0(N))\n\nprint(\"Bootstrapped standard errors: %g\" % np.std(b_draws))\nprint(\"Asymptotic approximation: %g\" % sigma_0(N))\nprint(\"Critical value for J statistic: %g (5%%)\" % limiting_J.isf(.05))"]},{"cell_type":"markdown","metadata":{},"source":["Now construct probability plot (bootstrapped $b$s vs. quantiles of\nlimiting distribution):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import probplot\n\n_ = probplot(b_draws,dist=limiting_b,fit=False,plot=plt)"]},{"cell_type":"markdown","metadata":{},"source":["Next, consider the a $p$-$p$ plot for $J$ statistics (recall these\nshould be distributed $\\chi^2_{\\ell-1}$).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def ppplot(data,dist):\n    data = np.array(data)\n\n    # Theoretical CDF, evaluated at points of data\n    P = [dist.cdf(x) for x in data.tolist()]\n\n    # Empirical CDF, evaluated at points of data\n    Phat = [(data<x).mean() for x in data.tolist()]\n\n    fig, ax = plt.subplots()\n    \n    ax.scatter(P,Phat)\n    ax.plot([0,1],[0,1],color='r') # Plot 45\n    ax.set_xlabel('Theoretical Distribution')\n    ax.set_ylabel('Empirical Distribution')\n    ax.set_title('p-p Plot')\n\n    return ax\n    \n_ = ppplot(J_draws, limiting_J)"]},{"cell_type":"markdown","metadata":{},"source":["## Example use of GMM class\n\n"]},{"cell_type":"markdown","metadata":{},"source":["I&rsquo;ve turned the code we developed last week into a class.  Here&rsquo;s a\nsimple example which  estimates a Mincer wage regression using data\nfrom Angrist-Krueger (1991):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n\nak = pd.read_stata('angrist-krueger91.dta')\n\ny = ak.logwage\nX = ak[['ageq','edu','married']]\nX['Constant'] = 1"]},{"cell_type":"markdown","metadata":{},"source":["Next we define a function that has the property that it&rsquo;s expected\nvalue at a true vector of parameters is equal to zero.  First, just a\nsimple Mincer wage regression:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def linear_regression(b,data):\n    \"\"\"\n    Return matrix X.T*e (ell x N)\n    \"\"\"\n    y,X=data\n    e = y - X@b\n\n    return (X.T*e).T"]},{"cell_type":"markdown","metadata":{},"source":["Import the GMM class\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from gmm import GMM\n\nGMM?"]},{"cell_type":"markdown","metadata":{},"source":["Now instantiate the GMM problem for this restriction, using the\nAngrist-Krueger data.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mygmm = GMM(linear_regression,(y,X),4)"]},{"cell_type":"markdown","metadata":{},"source":["## GMM Estimation of Logit Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider a &ldquo;logit&rdquo; regression; score function from MLE is:\n  $$\n     \\frac{1}{N}\\sum_j X_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n  $$\n  Let $u_j = y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}$; if we have some $Z$ such that\n  $\\mbox{E}(u|Z) = 0$ then we can construct further moment conditions\n  $$\n     \\frac{1}{N}\\sum_j Z_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n  $$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### GMM Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n\n# Some different options for moments...\n\ndef mle(b,data):\n    \"\"\"Observations of score for MLE estimator.\n\n    Moment condition is E(y_j - p(x_jb))x_j = 0,\n    where p(xb) = e^{xb}/(1+e^{xb})\n    \"\"\"\n    # Orient so that data are matrices with N rows\n    y,X,Z = data\n    N = np.max(y.shape)\n    y = y.reshape((N,-1))\n    X = X.reshape((N,-1))\n    Z = Z.reshape((N,-1))\n    \n    # Construct vector of identical parameter if b a scalar.\n    if np.isscalar(b) or len(b)==1: b = np.array([b]*X.shape[1])\n    b = np.array(b)\n    k = np.max(b.shape)\n    b = b.reshape((k,1))\n\n    p = np.exp(X@b)  # This is actually the odds\n    p = p/(1+p)      # This is probability y=1\n\n    return (X*(y - p))\n\ndef nonlinear_iv(b,data):\n    \"\"\"Observations for restriction that Z\n    orthogonal to score.\n\n    Moment condition is E(Z_jy_j - Z_jexp(x_jb)) = 0\n    \"\"\"\n    y,X,Z = data\n    # Orient so that data are matrices with N rows\n    N = np.max(y.shape)\n    y = y.reshape((N,-1))\n    X = X.reshape((N,-1))\n    Z = Z.reshape((N,-1))\n\n    # Construct vector of identical parameter if b a scalar.\n    if np.isscalar(b) or len(b)==1: b = np.array([b]*X.shape[1])\n    b = np.array(b)\n    k = np.max(b.shape)\n    b = b.reshape((k,1))\n\n    p = np.exp(X@b)  # This is actually the odds\n    p = p/(1+p)      # This is probability y=1\n\n    return (Z*(y - p))"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generating Process\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef dgp(N,beta,VXZ,gamma=1):\n    \"\"\"Generate a tuple of (y,X,Z).\n\n    Satisfies model:\n        Pr(y=1|X) = f(X@beta,gamma)\n        u = y - f(X@beta,gamma)\n        E(u|Z) = 0\n        f(x,gamma) = (exp(x)/(1+exp(x)))**gamma\n        Var([X,Z}) = VXZ\n        X,Z mean zero, Gaussian\n\n    Each element of the tuple is an array of N observations.\n    When gamma=1 this reduces to the logit model\n\n    Inputs include\n    - beta :: Governs effect of X on probability y=1\n    - gamma :: Governs curvature of function\n    - VXZ :: Var([X,Z])\n    \"\"\"\n    \n    # \"Square root\" of VXZ via eigendecomposition\n    lbda,v = np.linalg.eig(VXZ)\n    SXZ = v@np.diag(np.sqrt(lbda))\n\n    # Generate normal random variates [X*,Z]\n    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n\n    # Add a constant to both X & Z\n    X = np.c_[np.ones((N,1)),XZ[:,[0]]]\n    Z = np.c_[np.ones((N,1)),XZ[:,1:]]\n\n    # Calculate y\n    pi = np.exp(X.dot(beta))\n    pi = (pi/(1+pi))**gamma\n\n    y = iid.bernoulli(pi).rvs()\n\n    return y,X,Z"]},{"cell_type":"markdown","metadata":{},"source":["### The Truth (Mark I)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this version of the truth the form of the distribution of $y$ is\nknown up to the parameter $\\beta$; MLE takes advantage of this.\n\nChoose some parameters to establish the &ldquo;truth&rdquo;:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\n## Play with us!\nbeta = np.array([0,2])     # \"Coefficient of interest\"\n\n## Play with us!\n\n# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n\nell = 2 # Play with me too!\n\n# Arbitrary (but deterministic) choice for VXZ\nA = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n\n## Below here we're less playful.\n\n# Var([X,Z]|u) is constructed so that pos. def.\nVXZ = A.T@A \n\ntruth = (beta,VXZ)"]},{"cell_type":"markdown","metadata":{},"source":["### Monte Carlo Analysis of MLE via GMM\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we take the score to be our moment condition; $Z$ doesn&rsquo;t appear,\nso estimator is just identified.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gmm # Just code defined last class\n\ngmm.gj = mle  # Redefine gj function to use MLE score\n\ndata = dgp(1000,*truth)\n\nbhat,Jhat = gmm.two_step_gmm(data,b_init=np.zeros((2,1)))\n\nlimiting_J = iid.chi2(0)\n\nprint(\"b=(%f,%f), J=%f, Critical J=%f\" % (bhat[0],bhat[1],Jhat,limiting_J.isf(0.05)))"]},{"cell_type":"markdown","metadata":{},"source":["Now our experiment begins.  We set our frequentist hats firmly on our\nheads, and draw repeated samples of data, each generating a\ncorresponding estimate of beta.  Then the empirical distribution of\nthese samples tells us about the *finite* sample performance of our estimator.\n\nWe&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\nsamples of size $N$, until estimates of the covariance of our\nestimates converge:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gmm # Just module we discussed in class, but we're using it\n           # procedurally instead of as a class.\n\nN = 1000 # Sample size\n\ntol = 1e-3\n\nb_mle = []\nJ_mle = []\nb_nliv = []\nJ_nliv = []\n\nd=0\noldV = 0\nnewV = 1\nb = np.zeros((2,1))\nwhile d<30 or np.linalg.norm(oldV-newV) > tol:\n    d += 1\n    oldV = newV\n    data = dgp(N,*truth)\n    gmm.gj = mle\n    b,J = gmm.two_step_gmm(data,b_init=b)\n    b_mle.append(b)\n    J_mle.append(J)\n\n    gmm.gj = nonlinear_iv\n    b,J = gmm.two_step_gmm(data,b_init=b)\n    b_nliv.append(b)\n    J_nliv.append(J)\n\n    newV = np.var(b_nliv)"]},{"cell_type":"markdown","metadata":{},"source":["Now compare MLE & NLIV estimates:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n\n_ = plt.scatter(b_mle,b_nliv)"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Non-Linear Functions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Preface\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Some modules we&rsquo;ll want:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import norm\nfrom scipy.stats import distributions as iid\nimport pandas as pd\nimport matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Factories to generate basis functions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We can use a collection of functions as a *basis* with which to represent\nan arbitrary function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Factory function for phi_k(x)\nphi_factory = lambda c,s=1: lambda x: np.exp(-(1/(2*s))*norm(x-c)**2)  # RBF\n# phi_factory = lambda c,s=1: lambda x: (x**c)/s  # Polynomial\n# phi_factory = lambda c,s=1: lambda x: 1/(1+np.exp(c-x))  # logistic\n\n# Also chose a domain over which we'll want to evaluate the unknown function\nDomain = np.linspace(0,2*np.pi,100).tolist()"]},{"cell_type":"markdown","metadata":{},"source":["Now use  this factory to generate a set of $K$ basis functions for our\nrepresentation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["## Or\nK=18\n\n# Divide up domain\nphis = {k:phi_factory(Domain[x]) for k,x in enumerate(range(1,len(Domain),len(Domain)//K))}\n\n# Gram matrix\n#phis = {k:phi_factory(X[k]) for k in range(K)}\n\nphis[0] = lambda x: 1 # Constant function"]},{"cell_type":"markdown","metadata":{},"source":["Plot the basis functions:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["for k in range(K):\n    plt.plot(Domain,[phis[k](x) for x in Domain])"]},{"cell_type":"markdown","metadata":{},"source":["### Data generating function\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let $f_0(X) = \\mbox{E}(y|X)$ be an arbitrary function.  \nAfter specifying this function we define a data data-generating\nfunction for $(X,y)$ satisfying $y=f(X) + u$ and $\\mbox{E}(u|X)$.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["f0 = lambda x: x*np.sin(x) # True function; make up your own!\n\ndef dgp(N,sigma_u):\n    X = iid.uniform(loc=0,scale=2*np.pi).rvs(N).tolist()\n    X.sort()\n\n    u = iid.norm(scale=sigma_u)\n\n    y = pd.Series([f0(x) + u.rvs(1)[0] for x in X])\n\n    return X,y\n\nN = 20\nX,y = dgp(N,1)"]},{"cell_type":"markdown","metadata":{},"source":["Consider scatterplot:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nax.scatter(X,y)\n\nax.plot(Domain,[f0(x) for x in Domain])"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now regression.  Suppose we don&rsquo;t know the true function `f0`, and\ncan only estimate using observed data.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["TX = {}\nfor k in phis.keys():\n    TX[k] = [phis[k](x) for x in X]\n\nTX = pd.DataFrame(TX)\n\ntry: # If y isn't a DataFrame make it one\n    y = pd.DataFrame({'y':y})\nexcept ValueError: # Guess it is!\n    pass\n\nalpha = pd.DataFrame(np.linalg.lstsq(TX, y,rcond=None)[0],index=TX.columns)\n\n# Check fit:\ne = y.squeeze() - (TX@alpha).squeeze()\ne.var()"]},{"cell_type":"markdown","metadata":{},"source":["Note that expected *within* sample error variance is effectively zero!\n\nNow construct $\\hat{f}$ and plot predictions:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def fhat(x,alpha):\n\n    try: # Make alpha 1-d for calculations here\n        alpha = alpha.squeeze()\n    except AttributeError: # Maybe a list?\n        pass\n    \n    yhat = 0\n    for k,phik in phis.items():\n        yhat += alpha[k]*phik(x)\n\n    return yhat\n\nDomain = np.linspace(0,2*np.pi,100).tolist()"]},{"cell_type":"markdown","metadata":{},"source":["Plot me!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = ax.plot(Domain,[fhat(x,alpha) for x in Domain])"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluating Fit\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We&rsquo;d like a measure of how well we&rsquo;ve done in estimating the\nconditional expectation $f$.  First, compute the mean squared error\n(MSE).  Note that when we don&rsquo;t know the true function `f_0` that this\nis isn&rsquo;t feasible to compute.  Here we compute a sum of squared\nprediction errors as a crude way of computing the integral $$ \\int\n(f_0(x) - \\hat{f}(x))^2dx.  $$ This Riemann integral is appropriate\nhere because $x$ is uniformly distributed&#x2013;in general we want to\nintegrate with respect to $dF$.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dx = Domain[1]-Domain[0]\nMSE = np.sum([((f0(x) - fhat(x,alpha))**2)*dx for x in Domain])\n\nMSE"]},{"cell_type":"markdown","metadata":{},"source":["Note that the disturbances $u$ don&rsquo;t\nenter here&#x2014;this is the mean squared *approximation* error, not the\nmean squared error of predictions of actual realizations of the data\n$(y,X)$.  If we wanted the latter we&rsquo;d compute\n$$\n   \\mbox{E}((y-\\hat{f}(X))^2) =  \\mbox{E}((y-f(X)-\\epsilon)^2) =\n\\mbox{E}u^2 + \\mbox{MSE}.\n$$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Leave-one-out estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We next estimate $f$ using the &ldquo;leave-one-out&rdquo; approach.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Yminus = []\nAlpha = {}\nfor j in range(N):\n    alphaminus = pd.DataFrame(np.linalg.lstsq(TX[TX.index!=j],\n                                         y[TX.index!=j],rcond=None)[0],index=TX.columns)\n    yminus = lambda x: fhat(x,alphaminus)\n    Alpha[j]=alphaminus.squeeze()\n    Yminus.append(yminus)\n\nAlpha = pd.DataFrame(Alpha)\n\nyhat = lambda x: np.array([yminus(x) for yminus in Yminus]).mean()"]},{"cell_type":"markdown","metadata":{},"source":["Take a look at $f$ and the leave-one-out estimator:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n\nax.scatter(X,y,label='Data')\n\nax.plot(Domain,[f0(x) for x in Domain],label='$f_0$')\n\nax.plot(Domain,[fhat(x,alpha) for x in Domain],label='$\\hat{f}$')\n\nax.plot(Domain,[yhat(x) for x in Domain],label='$\\hat{y}_{-}$')\nax.legend()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n\nax.plot(Domain,[yhat(x) - fhat(x,alpha) for x in Domain],label='$\\hat{y}(x) - \\hat{f}(x)$')"]},{"cell_type":"markdown","metadata":{},"source":["#### Cross-validation error\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The cross-validation criterion relies on the leave-one-out estimator\n      $$\n          \\mbox{CV} = \\frac{1}{N}\\sum_{j=1}^N e_{-j}^2.\n      $$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["CV = 0\nfor j in range(N):\n     CV += (y.squeeze()[j] - Yminus[j](X[j]))**2/N"]},{"cell_type":"markdown","metadata":{},"source":["## Zeros in expenditure data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the following data from Uganda, collected at the household\n  level.  The data itself is *recall* data; the respondent is asked to\n  recall the value, the quantity, and the price of consumption out of\n  expenditures over the past week, for a rather long list of possible\n  non-durable expenditure items.  I&rsquo;ve organized the data as an array,\n  with each row corresponding to a household, and each column\n  corresponding to a different consumption item.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n\nx = pd.read_parquet('uganda_expenditures.parquet')"]},{"cell_type":"markdown","metadata":{},"source":["One thing to note about these data is the large number of &ldquo;zeros&rdquo;.\n  This may reflect the fact that few households consume all different\n  kinds of consumption goods every week, or could reflect &ldquo;missing&rdquo;\n  data on non-zero expenditures (e.g., if the respondent forgot).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Count of non-missing observations by year (t) and market (mkt)\nx.groupby(['t','mkt']).count().T"]},{"cell_type":"markdown","metadata":{},"source":["Missing data can cause serious problems in a demand analysis,\n   depending on how and why data might be missing.  If observations\n   are &ldquo;missing at random&rdquo; (MAR) then it may be an easy issue to\n   address, but if the probability of being missing is related to the\n   disturbance term in the demand equation this becomes a sort of\n   selection problem that will complicate estimation and inference.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Household characteristics\n\n"]},{"cell_type":"markdown","metadata":{},"source":["One class of variables that may help to explain zeros are\n   &ldquo;household characteristics&rdquo;; this includes household size and\n   composition (both because this affects demand and perhaps because\n   there are more potential shoppers); whether a household is urban or\n   rural, and perhaps other characteristics.\n\nHere are some characteristics for the households in Uganda:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["z = pd.read_parquet('uganda_hh_characteristics.parquet')\nz"]},{"cell_type":"markdown","metadata":{},"source":["### Data mining\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Unfortunately, demand theory doesn&rsquo;t offer much guidance to let us\n   know how household characteristics should be related to the\n   probability of a goods&rsquo; consumption being positive in a given week;\n   this is a case where a certain amount of &ldquo;data mining&rdquo; may be a\n   reasonable approach.\n\nWe&rsquo;ll use tools we&rsquo;ve discussed in class, relying on an\nimplementation given by the `scikit.learn` project.  In the first\ninstance, let&rsquo;s consider simply estimating a logit, where the\ndependent variable is simply a dummy indicating that the\nexpenditure of a given good $i$ for a household $j$ at time $t$ is\npositive, and where the right-hand-side variables are all the\nhousehold characteristics in `z`, combined with a collection of\ntime dummies (which we can think of as picking up the influence of\nprices, among other things):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n\ntime_effects = pd.get_dummies(z.reset_index()[['t']].set_index(z.index),columns=['t'])\n\nX = pd.concat([z,time_effects],axis=1).dropna(how='any') # Drop missing data\nx = x.dropna(how='all',axis=1)\n\n# Here's a good place to limit the number of dependent variables\n# if we want to save time.  We select just the first few columns:\nx = x.iloc[:,:5]\n\nEsts = {}\nfor item in x: # Iterate over dummies indicating positive expenditure\n    y = (x>0)[item]  # Dummy for non-missing item expenditures\n    Ests[item] = LogisticRegression(fit_intercept=False,penalty='none').fit(X,y)"]},{"cell_type":"markdown","metadata":{},"source":["#### Coefficients\n\n"]},{"cell_type":"markdown","metadata":{},"source":["This gives us a vector of coefficients for each good, which we can\nre-arrange into a pandas DataFrame.  Recall that in the logit model\n$e^{X\\beta}$ is interpreted as the *odds*.  Thus, for a variable in\n$X$ which is itself a logarithm, like log HSize, the associated\ncoefficient can be interpreted as an elasticity.  Accordingly, if the\ncoefficient on log HSize in the regression involving Matoke is 0.6,\nthen we can say that for every one percent increase in household size\n(other things equal) there&rsquo;s roughly a 0.6% increase in the odds of\nobserving positive Matoke consumption.  \n\nCoefficients associated with variables in levels have the\ninterpetation of *semi-elasticities*; thus, the odds of a rural\nhousehold consuming Matoke are approximately 53% less than that for\nthe average household in the sample.  What is the interpretation of\nthe coefficients associated with discrete counts of different\nhousehold members?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Coefs = pd.DataFrame({i:Ests[i].coef_.squeeze() for i in Ests.keys()},index=X.columns)\nCoefs"]},{"cell_type":"markdown","metadata":{},"source":["#### Cross-Validation & Lasso\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Interpreting the coefficients above allows us to think about how\ndifferences in household characteristics affect the odds of consuming\na particular good, but our original concern was that the data might\nnot be *missing at random*, which could complicate subsequent\nestimation of a demand system.  \n\nHere we use Lasso & cross-validation to tune the Lasso penalty\nparameter to check which (if any) of our regressors is useful for\nout-of-sample prediction.  \n\nWe again use a canned routine from sklearn, `LogisticRegressionCV`.\nThis bundles both the Lasso penalty criterion and cross-validation\ntogether for us, and searches over a list of penalty parameters to\nminimize the EMSE, computed via $K$-fold cross-validation.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegressionCV\nimport numpy as np\n\nLambdas = np.logspace(-5,5,11)\n\nCVEsts = {}\nfor item in x: # Iterate over dummies indicating positive expenditure\n    print(item)\n    y = (x>0)[item]  # Dummy for non-missing item expenditures\n\n    # Use 5-fold cross-validation in computing CV statistics; using\n    # penalty 'l1' implies a lasso estimator.\n    CVEsts[item] = LogisticRegressionCV(fit_intercept=False,\n                                        Cs = 1/Lambdas,        # Penalty 1/lambdas to search over\n                                        cv=5,                 # K folds\n                                        penalty='l1',         # Lasso penalty\n                                        solver='liblinear',\n                                        scoring='neg_mean_squared_error', # (minus) our CV statistic\n                                        n_jobs=-1             # Number of cores to use (-1=all)\n                                       ).fit(X,y)\n\nCVCoefs = pd.DataFrame({i:CVEsts[i].coef_.squeeze() for i in CVEsts.keys()},index=X.columns)\nCVCoefs"]},{"cell_type":"markdown","metadata":{},"source":["We can see how the estimated coefficients vary with different choices\nof the penalty parameter $\\lambda$ ($=1/C$).  Consider just the\ncoefficients associated with estimation of the Matoke logit: If we try\n$P$ different values of the penalty parameter using $K$-fold\ncross-validation this will be $KP$ different estimates for every\nparameter.  We can average over the $K$ different folds to get a\nclearer picture of how coefficients vary with &lambda;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["pd.DataFrame(CVEsts['Matoke'].coefs_paths_[True].mean(axis=0),index=Lambdas.tolist(),columns=X.columns).T"]},{"cell_type":"markdown","metadata":{},"source":["and see also how the EMSE varies with $\\lambda$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["EMSEs={k:-e.scores_[True].mean(axis=0).ravel() for k,e in CVEsts.items()} \n\nEMSEs = pd.DataFrame(EMSEs,index=np.log(Lambdas).tolist()).T\nEMSEs"]},{"cell_type":"markdown","metadata":{},"source":["Plotting these versus $\\log\\lambda$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["EMSEs.T.plot()"]},{"cell_type":"markdown","metadata":{},"source":["Finding the minima of these curves gives estimates of the optimal\n&lambda;:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lambda_star = pd.Series({k:1/e.C_[0] for k,e in CVEsts.items()})\nlambda_star"]},{"cell_type":"markdown","metadata":{},"source":["Large values of &lambda; encourage parsimony in the selection of\nregressors, so it&rsquo;s not surprising to find that consumption items with\nlarge values of $\\lambda^*$  also have few regressors (this is the\nmagic of Lasso):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Lasso_outcomes = pd.DataFrame({'#Regressors':(np.abs(CVCoefs)>1e-5).sum(),\n                               'λ*':lambda_star})\nLasso_outcomes"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Networks to Model Effects of Household Composition on Consumption\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We&rsquo;ll use `tensorflow` and `keras` to construct a multi-layer neural\n  network.  The goal is to use data on household size and composition\n  to predict expenditures on different kinds of food.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Preface\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Begin by specifying a place on disk where the model can be stored,\nand then loading the modules we&rsquo;ll be using.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Directory where model & friends is stored...\nfn = '/home/ligon/tmp/hh_characteristics_on_food.model/'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cfe\nimport pickle"]},{"cell_type":"markdown","metadata":{},"source":["### Custom loss functions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In our application there are many observations on food expenditures\nwhich are zero or missing.  We don&rsquo;t think these are very informative,\nand don&rsquo;t want our model to try to &ldquo;explain&rdquo; these missing data.  So,\nwe construct a simple &ldquo;custom&rdquo; loss function which simply zeros out\nany loss associated with a missing observation.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\n\n\ndef no_missing_loss(y_true,y_pred,metric='mse'):\n    \"\"\"\n    Allow for missing values of y_true so these don't contribute to loss.\n    \"\"\"\n    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n    y_true = math_ops.cast(y_true, y_pred.dtype)    \n\n    miss = tf.cast(y_true==9999,dtype=float)\n    notmissing = tf.subtract(1.,miss)\n    \n    if metric in ['mse','mean_squared_error']:\n        losses = tf.square(y_true-y_pred)\n    elif metric in ['mae','mean_absolute_error']:\n        losses = tf.abs(y_true-y_pred)\n    else:\n        raise NotImplementedError(\"Metric %s not implemented.\" % metric)\n\n    d2 = tf.multiply(losses,notmissing)\n\n    N = tf.reduce_sum(notmissing)\n    if N>0:\n        return tf.reduce_sum(d2)/N\n    else:\n        return N\n\n# Define metric \"mean absolute error\".\n\ndef mae(y,yhat): return no_missing_loss(y,yhat,metric='mae')\n\n# Needed to save model later\ncustom_objects = {'no_missing_loss':no_missing_loss,\n                  'mae':mae}"]},{"cell_type":"markdown","metadata":{},"source":["### Get data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Pull in the data we wish to use.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def get_data(LSMS_Library = '~/Data/LSMS_Library/'):\n\n    #################################\n    ## Deal with Data\n    #################################\n\n    z = pd.read_parquet(LSMS_Library + 'Uganda/_/household_characteristics.parquet')\n\n    x = pd.read_parquet(LSMS_Library + 'Uganda/_/food_expenditures.parquet')\n\n    x = x.fillna(0)\n\n    x,z = cfe.df_utils.drop_missing([x,z])\n\n    x.columns = x.columns.str.replace(' ','_',regex=False)  # No spaces allowed\n    x.columns = x.columns.str.replace('(','',regex=False)  # Or parentheses\n    x.columns = x.columns.str.replace(')','',regex=False)  # Or parentheses\n    x.columns = x.columns.str.replace(',','',regex=False)  # Or commas\n    x.columns = x.columns.str.replace('?','',regex=False)  # Or questionmarks\n\n    #x = np.log(x + np.sqrt(x**2+1)) # arcsinh transformation\n    x = np.log(x)\n\n    # Replace -inf with finite sentinel value\n    x = x.replace(-np.inf,9999)\n\n    #x = keras.utils.normalize(x)\n\n    # Randomly shuffle input data\n\n    x = x.sample(frac=1)\n    z = z.loc[x.index]\n\n    # Construct dummies for (t,)\n    t = pd.get_dummies(z.reset_index()['t'])\n    t.index = z.index\n\n    inputdf = pd.concat([t,z],axis=1)\n\n    outputdf = x\n\n    return inputdf,outputdf"]},{"cell_type":"markdown","metadata":{},"source":["### Construct & Train Model\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def construct_model(Foods):\n    #################################\n    ## Construct model\n    #################################\n\n    inputs = keras.Input(shape=(inputdf.shape[1],),\n                         dtype=tf.float16,\n                         name='hh_characteristics')\n\n    features = layers.Dense(2*128,activation='relu')(inputs)\n    residual = features\n    #features = layers.Dropout(0.5)(features)\n    features = layers.Dense(2*128,activation='relu')(features)\n    features = layers.add([features,residual]) # \"Skip\" block\n    features = layers.BatchNormalization()(features)\n\n    features = layers.Dense(2*64,activation='relu')(features)\n    residual = features\n    #features = layers.Dropout(0.5)(features)\n    features = layers.Dense(2*64,activation='relu')(features)\n    features = layers.add([features,residual]) # \"Skip\" block\n    features = layers.BatchNormalization()(features)\n\n    features = layers.Dense(2*32,activation='relu')(features)\n    residual = features\n    #features = layers.Dropout(0.5)(features)\n    features = layers.Dense(2*32,activation='relu')(features)\n    features = layers.add([features,residual]) # \"Skip\" block\n    features = layers.BatchNormalization()(features)\n\n    features = layers.Dense(2*32,activation='relu')(features)\n\n\n    # Output layer\n    outputs = [layers.Dense(1,name=s)(features) for s in Foods]\n\n\n    model = keras.Model(inputs = inputs,\n                        outputs = outputs)\n\n    model.compile(optimizer=\"rmsprop\",\n                  loss = no_missing_loss,\n                  metrics = [mae])\n\n    return model\n\ndef train(model,inputdf,outputdf):\n    #################################\n    ## Train model\n    #################################\n\n    callbacks_list = [] #[keras.callbacks.EarlyStopping(restore_best_weights=True, patience=5)]\n\n\n    history = model.fit(x=inputdf,\n                        y=outputdf,\n                        epochs = 100,\n                        validation_split=0.3,\n                        batch_size=64,\n                        callbacks=callbacks_list)\n\n    return history,model"]},{"cell_type":"markdown","metadata":{},"source":["### Main (for training model to predict log expenditures)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["if False: # __name__=='__main__':\n    inputdf,outputdf = get_data()\n\n    inputdf.to_parquet(fn+'inputdf.parquet')\n    outputdf.to_parquet(fn+'outputdf.parquet')\n\n    # Choose outputs\n    Foods = [s for s in outputdf.columns.tolist() if 'Matoke' in s]  # Several categories of matoke\n    Foods += ['Infant_Formula']\n    Foods = [s for s in outputdf.columns.tolist()]\n\n    model = construct_model(Foods)\n\n    history,model = train(model,inputdf,{k:outputdf[k] for k in Foods})\n\n    model.save(fn)\n\n    with open(fn + 'history.pickle','wb') as f:\n        pickle.dump(history.history,f)"]},{"cell_type":"markdown","metadata":{},"source":["### Main (for training model to predict CFE errors)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["if __name__=='__main__':\n    inputdf,outputdf = get_data()\n\n    # Linear estimation of CFE model\n    try:\n        r = cfe.from_dataset(fn + 'cfe_result.ds')\n    except IOError:\n        z = inputdf.filter(regex=' ')\n        z['m'] = 'Uganda'\n\n        z = z.reset_index().set_index(['j','t','m'])\n\n        y = outputdf.replace(9999,np.nan)\n        y['m'] = 'Uganda'\n\n        y = y.reset_index().set_index(['j','t','m'])\n\n        r = cfe.Result(y=y,z=z)\n        r.iterated_estimation()\n        r.to_dataset(fn + 'cfe_result.ds')\n\n    # Get Linear CFE residuals\n    e = r.e.to_dataframe().squeeze().unstack('i').dropna(how='all').droplevel('m')\n    e.index.names = ['j','t']\n\n    e,inputdf = cfe.df_utils.drop_missing([e.fillna(9999),inputdf])    # Choose outputs\n\n    e = e.astype('float32')\n\n    Foods = [s for s in e.columns.tolist()]\n\n    model = construct_model(Foods)\n\n    history,model = train(model,inputdf,{k:e[k] for k in Foods})\n\n    model.save(fn)\n\n    with open(fn + 'history.pickle','wb') as f:\n        pickle.dump(history.history,f)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Here&rsquo;s a a function to plot loss or other metrics, with a focus on\ncomparing validation loss to training loss.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["<<preface>>\n\ndef plot_validation_metric(history,metric='loss',total=False,log=False):\n    h = history\n    if not total:\n        H = [(k,v) for k,v in h.items() if metric in k]\n    else:\n        H = [(k,v) for k,v in h.items() if k in [metric,'val_%s' % metric]]\n\n    epochs = range(len(H[0][1]))\n    fig, ax = plt.subplots()\n    for i in H:\n        y = i[1]\n        if log: y = np.log(y)\n        ax.plot(epochs, y,label=i[0])\n        ax.set_xlabel(\"Epochs\")\n        if not log:\n            ax.set_ylabel(metric)\n        else: \n            ax.set_ylabel('log '+metric)\n\n    ax.legend()\n\n    return ax"]},{"cell_type":"markdown","metadata":{},"source":["Here are some functions to recover simple ways to evaluate the\nfunction or its gradients.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def input_gradient(model,x):\n    \"\"\"Gradient of predicted outputs w.r.t. inputs x.\n    \"\"\"\n\n    x = pd.DataFrame(x).T\n\n    myx = tf.Variable(x)\n\n    d = {}\n    with tf.GradientTape(persistent=True,watch_accessed_variables=False) as tape:\n        tape.watch(myx)\n        y = model(myx)\n\n    for j,name in enumerate(model.output_names):\n        d[name]=tape.gradient(y[j],myx).numpy().squeeze()\n\n    return pd.DataFrame(d,index=x.columns)\n\ndef vary_input(model,x,name,domain):\n    \"\"\"Return model predictions when varying input `name` over domain,\n       holding other inputs fixed at x0.\n    \"\"\"\n    def my_x(xi,x,name):\n        x0 = x.copy().squeeze()\n        x0[name] = xi\n        return tf.Variable(pd.DataFrame(x0).T)\n\n    y = []\n    for v in domain:\n        out = model(my_x(v,x,name))\n        y.append([foo.numpy() for foo in out])\n\n    return np.array(y).squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["Now, actually load a model and consider some evaluations of it.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Load model, with custom objects\n\nfrom train_nn import custom_objects, get_data\n\ninputdf = pd.read_parquet(fn + 'inputdf.parquet')\ny = pd.read_parquet(fn + 'outputdf.parquet')\n\nwith open(fn + 'history.pickle','rb') as f:\n    historyd = pickle.load(f)\n\nmodel = keras.models.load_model(fn,custom_objects=custom_objects)\n\n# Plot validation graph\nplot_validation_metric(historyd,log=True,total=True)\n\nplt.show()\n\nyhat = pd.DataFrame(np.array(model.predict(inputdf)).squeeze().T,columns=model.output_names,index=y.index)\n\ny = y.replace(9999,np.nan)\nfoo = y.join(yhat,rsuffix='_hat')\n\n#fig,ax = plt.subplots(1,3)\n\n#foo.plot.scatter(x='Passion_Fruits_hat',y='Passion_Fruits',ax=ax[0])\n#foo.plot.scatter(x='Onions_hat',y='Onions',ax=ax[1])\n#foo.plot.scatter(x='Soda_hat',y='Soda',ax=ax[1])\n\n#plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Compare with Linear Regression\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import cfe\n\ninputdf,outputdf = get_data()\n\nz = inputdf.filter(regex=' ')\nz['m'] = 'Uganda'\n\nz = z.reset_index().set_index(['j','t','m'])\n\ny = outputdf.replace(9999,np.nan)\ny['m'] = 'Uganda'\n\ny = y.reset_index().set_index(['j','t','m'])\n\nr = cfe.Result(y=y,z=z)\nr.get_beta()\n\nr['loglambdas'] = r['loglambdas']*0\n\ny_lin = r.get_predicted_log_expenditures()\ny_lin = y_lin.squeeze().to_dataframe()['yhat'].unstack('i').dropna(how='all')\nfoo.join(y_lin,rsuffix='_lin')"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}