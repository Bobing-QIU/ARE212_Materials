#+title: ARE212 Final Exam
#+author: Ethan Ligon
#+email: ligon@berkeley.edu
#+LATEX_HEADER: \newcommand{\T}{\top}
#+LATEX_HEADER: \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LaTeX_HEADER: \usepackage{fullpage}
#+LaTeX_HEADER: \renewcommand{\thesection}{\Roman{section}}
#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:{} arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK")
#+options: date:t e:t email:nil f:t inline:t num:t p:nil pri:nil
#+options: prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t
#+options: toc:nil todo:t |:t
#+options: H:1
#+language: en
#+select_tags: export
#+exclude_tags: noexport

* Introduction                                                       :ignore:
  This is the final exam for ARE212, covering material from the second
  half of the course taught in Spring 2021.   The exam is
  "take-at-home"; you may consult any resources you wish in completing
  it (notes, textbooks, lecture videos, etc.) except for other
  people.  This last restriction isn't easily enforceable; I rely on
  you to approach this as principled adults who adhere to the
  Berkeley Honor Code.

  More guidance:
  - The exam is due at 11am  on Tuesday May 11.
  - In completing the exam you should develop written arguments
    (e.g., expressed using \LaTeX{} or pencil and paper).  In some cases
    you may wish to supplement these written arguments with
    computation, such as Monte Carlo experiments.  Should you do so,
    please provide me with your working, open source, well-documented code.  (This
    last could be links to a github repo, a Jupyter notebook attached
    to an email, or similar).  In any case please be sure that
    materials you submit are well-organized and clearly
    documented---if I overlook some file you've sent or can't run it
    that's on you.
  - You are welcome (and even encouraged) to use arguments developed in our =piazza=
    discussions, but in this case please clearly cite the person and
    discussion (e.g., "As argued by Aaron in a discussion `Tests of
    Normality' (=@32_f3=)
    the optimal weighting matrix can be written as a function of a
    single unknown parameter.")
  - Please email files or links to =ligon@berkeley.edu=.
  - If you have questions about the final I will look for these on the
    #are212-econometrics channel of the [[https://app.slack.com/client/T01A34D5B6V/C01KC6ZBG4C][FY slack instance]], but I do
    not intend to be continuously available on-line, so much better if
    you can ask questions early!

* Identifying assumptions for regression
  For each of the questions in this section provide a short answer and
  argument.  Note the quality and concision of the argument 
  matters much more than the answer!
  1. Evaluate the truth of following statement: "In the linear
     regression $y=X\beta + u$ the usual identifying assumption
     $\E(u|X)=0$ implies $\E(h(X)\cdot u)=0$ for any function $h$
     satisfying some regularity conditions related to measurability."
  2. Consider the same linear regression $y=X\beta + u$, but now
     suppose an alternative identifying assumption $\E(X|u)=0$.  
     Construct a simple estimator based on this alternative.  Compare
     the usual and alternative identifying assumptions; are they
     equivalent?  Is one stronger than the other?
  3. Suppose that $y=f(X)+u$ for some unknown but continuous function
     $f$.  Suppose we want to use observed data on $X$ to predict
     outcomes $y$, and seek a predictor $\hat{y}(X)$ which is "best"
     in the sense that the mean squared prediction error $\E
     (y-\hat{y}(X)|X)^2$ is minimized.  What can we say about
     $\hat{y}$ and its relation to the conditional expectation
     $\E(y|X)$?  Its relation to $u$?

* Omitted Variables
  You are asked to serve as a referee for a paper submitted to a top
  field journal.  In the submitted paper the researcher uses a sample
  of size $N$ to estimate a model
  \[
     y = \alpha + \beta x + u.
  \]
  The coefficient $\beta$ seems to be significantly different from
  zero, but the researcher is concerned about omitted variable bias,
  so they also estimate a variety of alternative specifications of the form
  \[
     y = \alpha + \beta x + \gamma w + u,
  \]
  where $w$ is one of a number of other variables that the researcher
  hypothesizes might have some effect on $y$ as a way of testing the
  first model.

  The researcher finds a particular variable $w$ which enters the
  regression significantly, and so (i) rejects the first model,
  concluding that the first estimate of \beta was in fact affected by
  omitted variable bias; (ii) declares the augmented regression to be
  their "preferred specification;" and (iii) proceeds to construct
  standard \(t\)-statistics for $\beta$ and $\gamma$ as a way of
  proceeding with inference.

  Peer reviews in economics usually include some "notes for the
  author."  What might your notes say about the paper's approach to
  omitted variable bias?  Comment specifically on each of (i), (ii),
  and (iii).  Try to make your remarks critical yet
  constructive---what shortcomings do you see, and how might the
  author address these?

* Breusch-Pagan Extended
  Consider a linear regression of the form
  \[ 
     y = \alpha + \beta x + u,
  \]
  with $(y,x)$ both scalar random variables, where it is assumed that
  (a.i) $\E(u\cdot x) = \E u = 0$ and (a.ii) $\E(u^2|x)=\sigma^2$.  
  1. The condition a.i is essentially untestable, but
     cite:breusch-pagan79 argue that one can test a.ii via an
     auxiliary regression $\hat{u}^2 = c + d x + e$, where the $\hat{u}$
     are the residuals from the first regression, and the test of a.ii
     then becomes a test of $H_0:d=0$.  Explain both why a.i is untestable, and the logic of
     the test of a.ii.
  2. Use the two conditions a.i and a.ii to construct a GMM version of
     the Breusch-Pagan test.  
  3. What can you say  about the performance or relative merits of the
     Bruesch-Pagan test versus your GMM alternative?
  4. Suppose that in fact that $x$ is distributed uniformly over the
     interval $[0,2\pi]$, and $\E(u^2|x)=\sigma^2(x)=\sigma^2\sin(x)$, thus
     violating a.ii.  What can you say about the performance of the
     Breusch-Pagan test in this circumstance?  Can you modify your GMM
     test to provide a superior alternative?
  5. In the above, we've considered a test of a specific functional
     form for the variance of $u$.  Suppose instead that we don't have
     any prior information regarding the form of $\E(u^2|x)=f(x)$.
     Discuss how you might go about constructing an extended version
     of the Breusch-Pagan test which tests for $f(x)$ non-constant.
  6. Show that you can use your ideas about estimating $f(x)$ to
     construct a more efficient estimator of $\beta$ if $f(x)$ isn't
     constant.  Relate your estimator to the optimal generalized least
     squares (GLS) estimator.  

* Black Lives Matter
  cite:fryer19 uses data on encounters between police and civilians
  of different races in the US to explore how police use of force is
  related to a civilian's race.  While Fryer finds that Black and
  Hispanic civilians are much more likely to "experience some form of
  force" from the police and while the probability of being shot by the police is
  much higher for a civilian who is Black or Hispanic, Fryer's
  most prominent result is that for "the most extreme use of
  force---officer-involved shootings---we find no racial differences
  either in the raw data or when contextual factors are taken into
  account."  

  Introducing some notation, let $R$ denote the civilian's race; $U$
  some variables observed by the police officer prior to any
  interaction (e.g., observing "suspicious" behavior) but not the
  econometrician; $D$ a binary variable indicating the event ($D=1$) of an
  encounter between a given civilian and a police officer; $V$ a set
  of "contextual factors" related to the encounter and reported by the
  officer; and $S$ the event that the civilian is shot by the officer.
  We can then express Fry's finding regarding shootings as not being
  able to reject either
  \begin{equation}
  \label{eq:fry1}
      \mbox{Pr}(S|D=1,R) = \mbox{Pr}(S|D=1)
  \end{equation}
  or
  \begin{equation}
  \label{eq:fry2}
      \mbox{Pr}(S|D=1,V,R) = \mbox{Pr}(S|D=1,V).
  \end{equation}

   1. cite:durlauf-heckman20 criticize this conclusion of Fryer's, on
      the grounds that $D$ may be an endogenous variable.  You needn't
      read their paper, but explain in your own words what sorts of
      endogeneity might undermine Fryer's conclusion that the
      probability of being shot by the police doesn't depend on race.

   2. Spell out conditions on $(R,S,U,V,D)$ (perhaps using a causal diagram)
      which would suffice to interpret (1) and (2) as evidence
      that there are no racial differences in the victims of police
      shootings.  In particular, what does one need to assume about
      $\mbox{Pr}(D=1|R,U)$?

   3. Consider an alternative ("driving while Black") model in which
      the police use race as a criterion for stopping or otherwise
      interacting with a given civilian.  Compare the causal structure
      of this model with your answer to (2).  Viewed through the lens
      of this model, how would one interpret Fry's failure to reject
      $\mbox{Pr}(S|D=1,R) = \mbox{Pr}(S|D=1)$?

   4. The Justice Department should care[fn:: The Equal Protection
      Clause of the fourteenth amendment to the US constitution is
      generally interpreted to require "equality before the law" for
      all persons subject to the jurisdiction of the various states,
      and the adoption of this amendment shortly after the Civil War
      is regarded as evidence that it was specifically intended to
      prevent discrimination against Black Americans.] about which
      (Fry's or the "driving while Black") is the better model.  How
      might one go about testing one against the other?

* Weighting of Linear IV Estimators
  Consider the Linear IV model
  \[
      y = X\beta + u\qquad \E(Z^\T u)=0.
  \]
  1. Exploiting the moment condition, under what conditions does the
     estimator $b_{IV}$ satisfying $Z^\T y = (Z^\T X)b_{IV}$
     consistently estimate $\beta$?
  2. Suppose that $Z$ has $\ell$ columns.  Suppose that $W$ is a symmetric,
     $\ell\times\ell$ full rank matrix, with a corresponding estimator $b_W$
     satisfying $WZ^\T y = W(Z^\T X)b_{W}$.  Under what conditions
     will this estimator consistently estimate $\beta$?
  3. Describe the GMM criterion function that $b_W$ minimizes.
  4. Consider Hansen's description of the two-stage least squares
     estimator (Section 12.12).  What is $W$ for this estimator?
     Under what conditions is this the optimally weighted GMM estimator?
  5. $W=I$ for the $b_{IV}$ estimator described above.  Under what
     conditions is $b_{IV}$ the optimally weighted GMM estimator?
  6. For the estimator described in (2), suppose that $W$ is diagonal,
     with diagonal elements proportional to
     $(1,1/2,1/4,...,2^{1-\ell})$.  Under what conditions is the
     estimator $b_W$ the optimally weighted estimator?  Can you think
     of a practical example where the optimal weighting matrix might
     have this structure?


* RCT Design                                                       :noexport:
  When designing an RCT (randomized control trial), one important
  element of the experimental design involves /power calculations/;
  these in turn rely on pre-specification of the regression one
  proposes to estimate; this regression is generally supposed to
  identify one or more parameters of interest; often the question the
  experiment is designed to answer boils down to whether or not this
  parameter is different from zero, which suggests a test statistic
  (typically a $t$ statistic).

  So, one thing that needs to be settled early is how large the
  experiment needs to be to make the probability of a type II error
  less than some benchmark (typically 20%), holding fixed the
  probability of a type I error (typically 5%).  A large number of
  examples can be found at the AEA registry; one interesting case is the
  registration https://www.socialscienceregistry.org/trials/1558,
  which eventually led to publication as cite:bandiera-etal20.  This
  involves some randomly assigned /treatment/; in the example given
  this is a /community-level/ treatment involving the establishment of
  clubs for adolescent girls; the (alternative) hypothesis of the
  study is that the establishment of such clubs will lead to greater
  "economic empowerment" for participating girls and "greater control
  over their bodies".

  This particular study involved 150 communities, fifty of
  which were randomly assigned to be "controls", while 100 were randomly
  chosen to have clubs established within them.  Suppose that whether
  a girl $j$ lives in a community with a club depends on a binary
  treatment variable $T_j$.

  1. Suppose that we're interested in the effect of clubs on some
     outcome $y$, and so wish to estimate the parameter $\beta_1$ in
     $y = \beta_0 + T\beta_1 + u$.  The random assignment of $T$
     implies that it is independent of $u$.  Suggest a moment
     condition that could be exploited to estimate $\beta_1$.
  2. Suppose it is known in advance that the variance of $y$ is
     one.  The registration for this experiment indicates
     that about 4000 girls lived in treatment communities, while about
     2000 lived in control communities.  Under an iid sampling
     assumption, construct a \(t\)-statistic which could be used to
     test the hypothesis that the OLS estimate of $\beta_1$, say
     $b_{OLS}$, was significantly different from zero.
  3. Still using the OLS estimator and the iid sampling assumption,
     what is the "minimum detectable effect size" allowing for a
     probability of type I error of 5% and a probability of type II
     error of 20% (where the absolute value of $\beta_1$ is
     interpreted as the "effect size")?
  4. It is unlikely that all girls in treatment communities will
     actually join the "club"; instead, each will make a decision
     about whether to join or not; denote this by $D_j$ equal to one
     if girl $j$ joins the club, and zero otherwise.  If we're
     interested in the effects of club participation on outcome $y$ rather than the
     effects of having a club in the community, this suggests that the
     equation of interest ought to be something like $y=\gamma_0 +
     D\gamma_1 + v$.  The treatment $T$ is still randomly assigned,
     though of course $D$ is not; how can this be exploited to obtain
     estimates of $\gamma_1$?
  6. In the registration for the RCT, the researchers proposed using
     the randomly assigned treatment as an instrument for girls'
     participation decision, and construct a just-identified two-stage
     least squares estimator of the coefficient corresponding to our
     $\gamma_1$ (in the application there are other controls, with
     which we won't concern ourselves).  What can we say about the
     distribution of this estimator and distribution of the test
     statistic you employed to handle the power calculations?  If
     $\gamma_1$ is the coefficient of interest, how would you go about
     re-doing the power calculations?  What are the critical issues,
     and how could they be addressed?
  7. In addition to the moment conditions which identify the two-stage
     least squares estimator, the independence of $T$ implies that
     there are many more moment conditions which could be exploited.
     Suggest a /sequence/ of possible moment conditions, and indicate
     a practical estimation strategy which could make efficient use of these.
  8. The treatment $T$ is randomly assigned to different
     /communities/; obviously it is not randomly assigned to different
     /girls/ (the correlation between $(T_j,T_{j'})$ is one for girls
     $(j,j')$ in the same community).  Sketch a causal diagram (a
     directed graph) illustrating a set of assumptions sufficient for
     the two-stage least squares estimator to consistently estimate
     the model parameter $\gamma_1$.  Comment on the plausibility of
     these assumptions; are any of these testable?

* Nested Samples                                                   :noexport:
  Consider the linear model $y=X\beta + u$, where $X$ is thought to
  depend on $u$, but where we have a set of instruments $Z$ such that
  $\E Z^\T u = 0$.  In this case our observations on $y$ are limited,
  in that we don't always observe $y$ even when we do observe
  $(X,Z)$.  We can think of this as having two samples, nested in the
  following way.  We have $N_1$ iid observations on the triple
  $(y,X,Z)_1$ but $N_2>N_1$ iid observations on $(X,Z)_2$, with $(X,Z)_1$
  (i.e., the observations on $X$ and $Z$ in the first dataset) a
  subset of $(X,Z)_2$.  How can we best make use of all these data?
  1. One econometrician suggests an augmented sort of two-stage-least
     squares approach, using the richer dataset to estimate a
     linear relationship $X_2 = Z_2\pi + v$, and thus constructing a
     "first-stage" prediction equation $\hat{X} = Z\hat{\pi}_2$ which
     is more precisely estimated that it would be in the usual case in
     which only data in $(y,X,Z)_1$ was exploited.
     a. Continue the argument by substituting into the second stage.
        What can you say about the properties of the augmented
        estimator compared to the properties of the usual
        two-stage least squares estimator?
     b. Under what conditions would the augmented estimator be
        preferred to two-stage least squares on just the sample of
        $N_1$ observations?

  2. A second econometrician suggests using the smaller sample to
     construct a sample moment condition $(Z_1^\T y_1)=(Z_1^\T X_1)b$,
     and argues that if $b$ in this condition identifies $\beta$, then
     it should be possible to construct $\hat{u}_2 = \hat{y}_2 -
     X_2b$, and that for this larger set of observations we must have
     $\E Z_2^\T\hat{u}_2 = 0$.  She argues that these two sets of moment
     conditions could then be combined into an over-identified
     optimally-weighted GMM estimator.
     a. How would you construct the optimal GMM weighting matrix for
        this approach?  Derive an expression for the asymptotic
        variance matrix for the estimator $b$.  How does it depend on
        the larger sample?
     b. Comment on this approach.  Does the second set of moment
        conditions add useful information?
     c. If you also knew that $u$ was homoskedastic how could you
        exploit this information?  How would the resulting estimator
        compare with two-stage-least squares?  What can you say about
        the relative efficiency of this estimator versus two-stage
        least squares?

* Further ideas                                                    :noexport:
  - Exploit difference between mean independence & independence
  - "best predictor" defined as one that minimizes MSE; turns out to
    be conditional expectation.
  - Projection error vs prediction error (linear vs. nonlinear)
  - Inference after specification search (e.g., test for an omitted
    variable).
* Causality in models                                              :noexport:
  - Specify a simple model with some causal structure.
  - Describe an experiment with randomization, work out predicted outcome.
  - Do a Rubin; estimate "causal" effect.
  - Note difference; emphasis that "causality" inheres in the model.
* References                                                         :ignore:
\printbibliography
