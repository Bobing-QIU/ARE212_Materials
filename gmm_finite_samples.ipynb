{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Finite Sample Properties of Linear GMM\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["GMM provides a generalized way to think about instrumental\nvariables estimators, and we also have evidence that the finite\nsample properties of these estimators may be poor.  Here we&rsquo;ll\nconstruct a simple Monte Carlo framework within which to evaluate\nthe finite-sample behavior of GMM linear IV estimators.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Asymptotic Variance of GMM estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["If we have $\\mbox{E}g_j(\\beta)g_j(\\beta)^\\top=\\Omega$ and\n$\\mbox{E}\\frac{\\partial g_j}{\\partial b^\\top}(\\beta)=Q$ then we&rsquo;ve\nseen that the asymptotic variance of the optimally weighted GMM\nestimator is\n$$\n       V_b = \\left(Q^\\top\\Omega^{-1}Q\\right)^{-1}.\n   $$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generating Process\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We consider the linear IV model\n\n\\begin{align*}\n   y &= X\\beta + u\\\\\n   \\mbox{E}Z^\\top u = 0\n\\end{align*}\n\nThus, we need to describe processes that generate $(X,Z,u)$.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\n# Let Z have order ell, and X order 1, with Var([X,Z])=VXZ\n\nell = 4\n\n# Arbitrary (but deterministic) choice for VXZ\nA = np.sqrt(np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1))\nVXZ = A.T@A/100 \n\nQ = VXZ[1:,[0]]  # EZX'\n\nsigma_u = 1\n\nOmega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n\n# Asymptotic variance of optimally weighted GMM estimator:\nprint(inv(Q.T@inv(Omega)@Q))"]},{"cell_type":"markdown","metadata":{},"source":["Now code to generate N realizations of $(y,X,Z)$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef dgp(N,beta,sigma_u,VXZ):\n    \n    u = iid.norm.rvs(size=(N,1))*sigma_u\n\n    # \"Square root\" of VXZ\n    l,v = np.linalg.eig(VXZ)\n    SXZ = v@np.diag(np.sqrt(l))\n\n    # Generate normal random variates [X,Z]\n    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n\n    # But X is endogenous...\n    X = XZ[:,[0]] + u\n    Z = XZ[:,1:]\n\n    # Calculate y\n    y = X*beta + u\n\n    return y,X,Z"]},{"cell_type":"markdown","metadata":{},"source":["Check on DGP:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000\n\ny,X,Z = dgp(N,1,1,VXZ)\n\n# Check that we've computed things correctly:\nprint(np.cov(np.c_[X,Z].T) - VXZ)"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have a data-generating process we proceed with under\n   the conceit that we can observe samples generated by this process,\n   but otherwise temporarily &ldquo;forget&rdquo; the properties of the DGP, and use the\n   generated data to try to reconstruct aspects of the DGP.\n\nHere we consider using the optimally weighted linear IV estimator.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Construct sample moments\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Begin by defining a function to construct the sample moments given\n    the data and a parameter estimate $b$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def gj(b,y,X,Z):\n    \"\"\"Observations of g_j(b)\n    \"\"\"\n    return Z*(y - X*b)\n\ndef gN(b,y,X,Z):\n    \"\"\"Averages of g_j(b)\n    \"\"\"\n    u = gj(b,y,X,Z)\n    \n    return u.mean(axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["#### Define estimator of Egg'\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we define a function to compute covariance matrix of moments.\nRe-centering can be important in finite samples, even if irrelevant in\nthe limit.  Since we have $\\mbox{E}g_j(\\beta)=0$ under the null we may\nas well use this information when constructing our weighting matrix.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def Omegahat(b,y,X,Z):\n    u = gj(b,y,X,Z)\n\n    # Recenter! We have Eu=0 under null.\n    # Important to use this information.\n    u = u - u.mean(axis=0) \n    \n    return u.T@u/u.shape[0]\n\n# Check construction:\nWinv = Omegahat(.3,y,X,Z) \nprint(Winv)"]},{"cell_type":"markdown","metadata":{},"source":["Finally define the criterion function given a weighting matrix $W$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def J(b,W,y,X,Z):\n\n    m = gN(b,y,X,Z)\n    \n    return m.T@W@m*y.shape[0] # Scale by sample size\n\n# Check construction\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nlimiting_J = iid.chi2(ell-1)\n\nB = np.linspace(-0,2,100)\nW = inv(Winv)\n\nplt.plot(B,[J(b,W,y,X,Z) for b in B.tolist()])\nplt.axhline(limiting_J.isf(0.05),color='r')"]},{"cell_type":"markdown","metadata":{},"source":["#### Two Step Estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We next implement the two-step GMM estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize_scalar\n\ndef two_step_gmm(y,X,Z):\n\n    # First step uses identity weighting matrix\n    W1 = np.eye(Z.shape[1])\n\n    b1 = minimize_scalar(lambda b: J(b,W1,y,X,Z)).x \n\n    W2 = inv(Omegahat(b1,y,X,Z))\n\n    return minimize_scalar(lambda b: J(b,W2,y,X,Z))"]},{"cell_type":"markdown","metadata":{},"source":["Now let&rsquo;s try it with an actual sample:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["soltn = two_step_gmm(y,X,Z)\n\nprint(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"]},{"cell_type":"markdown","metadata":{},"source":["### Monte Carlo Draws\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\nsamples of size $N$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000\n\nD = 1000\n\nb_list = []\nJ_list = []\nfor d in range(D):\n    soltn = two_step_gmm(*dgp(N,1,sigma_u,VXZ))\n    b_list.append(soltn.x)\n    J_list.append(soltn.fun)\n\n_ = plt.hist(b_list,bins=int(np.ceil(np.sqrt(N))))"]},{"cell_type":"markdown","metadata":{},"source":["### Distribution of Monte Carlo draws vs. Asymptotic distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Compare Monte Carlo standard errors with asymptotic approximation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Limiting distribution of estimator\n\nsigma_0 = np.sqrt(inv(Q.T@inv(Winv)@Q)/N)[0][0] # Limiting Std.\nlimiting_b = iid.norm(scale=sigma_0)\n\nprint(\"Bootstrapped standard errors: %g\" % np.std(b_list))\nprint(\"Asymptotic approximation: %g\" % sigma_0)\nprint(\"Critical value for J statistic: %g (5%%)\" % limiting_J.isf(.05))"]},{"cell_type":"markdown","metadata":{},"source":["Now construct probability plot (bootstrapped $b$s vs. quantiles of\nlimiting distribution):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import probplot\n\n_ = probplot(b_list,dist=limiting_b,fit=False,plot=plt)"]},{"cell_type":"markdown","metadata":{},"source":["Next, consider the a $p$-$p$ plot for $J$ statistics (recall these\nshould be distributed $\\chi^2_{\\ell-1}$).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from statsmodels.api import ProbPlot\n\n# statsmodels is too good for lists, so create 1-d array\ntestplots = ProbPlot(np.array(J_list),limiting_J) \n_ = testplots.qqplots(line='45')\n_ = testplots.ppplots(line='45')"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}