% Created 2022-05-02 Mon 09:50
% Intended LaTeX compiler: pdflatex
\RequirePackage{rotating}
\documentclass[12pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[authordate-trad,backend=biber,natbib]{biblatex-chicago}
\usepackage{wasysym}
\newcommand{\Cov}{\ensuremath{\mbox{Cov}}}
\renewcommand{\Pr}{\ensuremath{\mbox{Pr}}}
\newcommand{\Eq}[1]{(\ref{eq:#1})}
\usepackage{bm}\usepackage{econometrics}
\newcommand{\T}{\top}
\newtheorem{proposition}{Proposition} \newcommand{\Prop}[1]{Proposition \ref{prop:#1}}
%\newtheorem{problem}{Problem} \newcommand{\Prob}[1]{Problem \ref{prob:#1}}
%\newtheorem{theorem}{Theorem} \newcommand{\Thm}[1]{Theorem \ref{thm:#1}}
%\newtheorem{corollary}{Corollary} \newcommand{\Cor}[1]{Corollary \ref{cor:#1}}
%\newtheorem{remark}{Remark} \newcommand{\Rem}[1]{Remark \ref{rem:#1}}
%\newtheorem{condition}{Condition} \newcommand{\Cond}[1]{Condition \ref{cond:#1}}
%\newtheorem{lemma}{Lemma} \newcommand{\Lem}[1]{Lemma \ref{lem:#1}}
%\newtheorem{assumption}{Assumption} \newcommand{\Ass}[1]{Assumption \ref{ass:#1}}
\newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
\usepackage{dsfont}\newcommand{\one}{\ensuremath{\mathds{1}}}
\usepackage{xcolor}
\newcommand{\rv}[1]{\ensuremath{\textcolor{red}{#1}{}}}
%\newcommand{\rv}[1]{\ensuremath{{}_{rv}{#1}{}}}
%\newcommand{\rv}[1]{\ensuremath{\underline{#1}{}}}
\newcommand{\rvy}{\rv{y}}
\newcommand{\rvX}{\rv{X}}
\newcommand{\rvx}{\rv{x}}
\newcommand{\rvu}{\rv{u}}
\renewcommand{\do}[1]{\ensuremath{\mbox{do}(#1)}}
\renewcommand{\E}{\ensuremath{\mathds{E}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\author{Ethan Ligon}
\date{\today}
\title{Exercises (Cross-Validation)}
\hypersetup{
 pdfauthor={Ethan Ligon},
 pdftitle={Exercises (Cross-Validation)},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.0.50 (Org mode 9.5.2)}, 
 pdflang={English}}
\begin{document}

\maketitle
Consider estimation of a linear model \(y = X\beta + u\), with the
identifying assumption that \(\E(u|X)=0\).  

When we compute \(K\)-fold cross-validation of a tuning parameter \(\lambda\)
(e.g., the penalty parameter in a LASSO regression), then for each value of
\(\lambda\) we obtain \(K\) estimates of any given parameter, say
\(\beta_i\); denote the estimates of this parameter by
\(b_{i}^\cdot=(b_{i}^1,\dots,b_{i}^K)\).  If our total sample (say
\(D_1\)) comprises
\(N\) iid observations, then each of our \(K\) estimates will be based
on a sample \(D_1^k\) of roughly \(N\frac{K-1}{K}\) observations.

\begin{enumerate}
\item How can you use the estimates \(b_{i}^\cdot\) to estimate the
variance of the estimator?

\item What can you say about the variance of your estimator of the
variance?  In particular, how does it vary with \(K\)?

\item Suppose we use \(\bar{b}(\lambda)=K^{-1}\sum_{k=1}^K b^{k}\) as our
preferred estimate of \(\beta\) at a given value of the tuning
parameter \(\lambda\).  Construct an \(R^2\) statistic which maps a
sample \(D\) and a parameter vector \(b\) into \([0,1]\).  Compare the
following:

\begin{enumerate}
\item \(R^2(D_1,\bar{b}(\lambda))\) and \(R^2(D_1,b_{OLS})\), where
\(b_{OLS}\) denotes the OLS estimator estimated using the entire
sample \(D_1\), so that \(R^2(D_1,b_{OLS})\) corresponds to the
usual least-squares \(R^2\) statistic.

\item \(R^2(D,\bar{b}(\lambda))\) and \(R^2(D,b_{OLS})\), where
\(b_{OLS}\) and \(\bar{b}(\lambda)\) are estimated using \(D_1\) as
described above, but where \(D\) is some other iid sample from
the same data-generating process.

\item \(K^{-1}\sum_{k=1}^K R^2(D_1^k,\bar{b}(\lambda))\) and
\(K^{-1}\sum_{k=1}^K R^2(D_1^k,b_{OLS})\);

\item \(K^{-1}\sum_{k=1}^K R^2(D_1^k,\bar{b}(\lambda))\) and
\(K^{-1}\sum_{k=1}^K R^2(D_1^k,b^{k}(\lambda))\);

\item \(R^2(D,\bar{b}(\lambda))\) and \(R^2(D,\beta)\);

\item \(R^2(D,b_{OLS})\) and \(R^2(D,\beta)\);
\end{enumerate}

\item How do the \(R^2\) statistics you worked with above compare with
various notions of mean-square error?  The statistics which rely
on \(\beta\) are typically infeasible, so setting these aside, how
might you use these statistics to choose a ``best'' estimator?
\end{enumerate}
\end{document}