{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeros in expenditure data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following data from Uganda, collected at the household\n",
    "level.  The data itself is *recall* data; the respondent is asked to\n",
    "recall the value, the quantity, and the price of consumption out of\n",
    "expenditures over the past week, for a rather long list of possible\n",
    "non-durable expenditure items.  I&rsquo;ve organized the data as an array,\n",
    "with each row corresponding to a household, and each column\n",
    "corresponding to a different consumption item.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_pickle('uganda_expenditures.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note about these data is the large number of &ldquo;zeros&rdquo;.\n",
    "  This may reflect the fact that few households consume all different\n",
    "  kinds of consumption goods every week, or could reflect &ldquo;missing&rdquo;\n",
    "  data on non-zero expenditures (e.g., if the respondent forgot).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of non-missing observations by year (t) and market (mkt) (transposed)\n",
    "# FIXME: there is only one value for mkt?\n",
    "x.groupby(['t','mkt']).count().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data can cause serious problems in a demand analysis,\n",
    "   depending on how and why data might be missing.  If observations\n",
    "   are &ldquo;missing at random&rdquo; (MAR) then it may be an easy issue to\n",
    "   address, but if the probability of being missing is related to the\n",
    "   disturbance term in the demand equation this becomes a sort of\n",
    "   selection problem that will complicate estimation and inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household characteristics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One class of variables that may help to explain zeros are\n",
    "   &ldquo;household characteristics&rdquo;; this includes household size and\n",
    "   composition (both because this affects demand and perhaps because\n",
    "   there are more potential shoppers); whether a household is urban or\n",
    "   rural, and perhaps other characteristics.\n",
    "\n",
    "Here are some characteristics for the households in Uganda:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_pickle('uganda_hh_characteristics.pickle')\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, demand theory doesn&rsquo;t offer much guidance to let us\n",
    "   know how household characteristics should be related to the\n",
    "   probability of a goods&rsquo; consumption being positive in a given week;\n",
    "   this is a case where a certain amount of &ldquo;data mining&rdquo; may be a\n",
    "   reasonable approach.\n",
    "\n",
    "We&rsquo;ll use tools we&rsquo;ve discussed in class, relying on an\n",
    "implementation given by the `scikit.learn` project.  In the first\n",
    "instance, let&rsquo;s consider simply estimating a logit, where the\n",
    "dependent variable is simply a dummy indicating that the\n",
    "expenditure of a given good $i$ for a household $j$ at time $t$ is\n",
    "positive, and where the right-hand-side variables are all the\n",
    "household characteristics in `z`, combined with a collection of\n",
    "time dummies (which we can think of as picking up the influence of\n",
    "prices, among other things):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "time_effects = pd.get_dummies(z.reset_index()[['t']].set_index(z.index),columns=['t'])\n",
    "# make dummies out of year values; reset_index()[['t']] makes a new df with the t index into a column, \n",
    "# while setting the index to be the same as the old df\n",
    "\n",
    "X = pd.concat([z,time_effects],axis=1).dropna(how='any') # Drop missing data\n",
    "# note that axis=1 on concat means we glued the time effects on similar to a merge or join; horiz. not vertically\n",
    "x = x.dropna(how='all',axis=1)\n",
    "\n",
    "# Here's a good place to limit the number of dependent variables\n",
    "# if we want to save time.  We select just the first few (5) columns (and all rows):\n",
    "x = x.iloc[:,:5]\n",
    "\n",
    "Ests = {}\n",
    "for item in x: # Iterate over dummies indicating positive expenditure\n",
    "    y = (x>0)[item]  # Dummy for non-missing item expenditures (turn into series of True, False on the condition x>0)\n",
    "    Ests[item] = LogisticRegression(fit_intercept=False,penalty='none').fit(X,y) \n",
    "    # save logit results per item to the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a vector of coefficients for each good, which we can\n",
    "re-arrange into a pandas DataFrame.  Recall that in the logit model\n",
    "$e^{X\\beta}$ is interpreted as the *odds*.  Thus, for a variable in\n",
    "$X$ which is itself a logarithm, like log HSize, the associated\n",
    "coefficient can be interpreted as an elasticity.  Accordingly, if the\n",
    "coefficient on log HSize in the regression involving Matoke is 0.6,\n",
    "then we can say that for every one percent increase in household size\n",
    "(other things equal) there&rsquo;s roughly a 0.6% increase in the odds of\n",
    "observing positive Matoke consumption.  \n",
    "\n",
    "Coefficients associated with variables in levels have the\n",
    "interpetation of *semi-elasticities*; thus, the odds of a rural\n",
    "household consuming Matoke are approximately 53% less than that for\n",
    "the average household in the sample.  What is the interpretation of\n",
    "the coefficients associated with discrete counts of different\n",
    "household members?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coefs = pd.DataFrame({i:Ests[i].coef_.squeeze() for i in Ests.keys()},index=X.columns)\n",
    "# make  dataframe where each column name is the key from Ests; values are the coefficients extracted from the results object\n",
    "# make the index the column names of X (which were our regressors)\n",
    "Coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation & Lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients above allows us to think about how\n",
    "differences in household characteristics affect the odds of consuming\n",
    "a particular good, but our original concern was that the data might\n",
    "not be *missing at random*, which could complicate subsequent\n",
    "estimation of a demand system.  \n",
    "\n",
    "Here we use Lasso & cross-validation to tune the Lasso penalty\n",
    "parameter to check which (if any) of our regressors is useful for\n",
    "out-of-sample prediction.  \n",
    "\n",
    "We again use a canned routine from sklearn, `LogisticRegressionCV`.\n",
    "This bundles both the Lasso penalty criterion and cross-validation\n",
    "together for us, and searches over a list of penalty parameters to\n",
    "minimize the EMSE, computed via $K$-fold cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n",
    "\n",
    "Lambdas = np.logspace(-5,5,11) # 11 evenly spaced numbers on the log scale from -5, 5\n",
    "\n",
    "CVEsts = {}\n",
    "for item in x: # Iterate over dummies indicating positive expenditure\n",
    "    print(item)\n",
    "    y = (x>0)[item]  # Dummy for non-missing item expenditures\n",
    "\n",
    "    # Use 5-fold cross-validation in computing CV statistics; using\n",
    "    # penalty 'l1' implies a lasso estimator.\n",
    "    CVEsts[item] = LogisticRegressionCV(fit_intercept=False,\n",
    "                                        Cs = 1/Lambdas,        # Penalty 1/lambdas to search over\n",
    "                                        cv=5,                 # K folds\n",
    "                                        penalty='l1',         # Lasso penalty\n",
    "                                        solver='liblinear',\n",
    "                                        scoring='neg_mean_squared_error', # (minus) our CV statistic\n",
    "                                        n_jobs=-1             # Number of cores to use (-1=all)\n",
    "                                       ).fit(X,y)\n",
    "\n",
    "CVCoefs = pd.DataFrame({i:CVEsts[i].coef_.squeeze() for i in CVEsts.keys()},index=X.columns)\n",
    "CVCoefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the estimated coefficients vary with different choices\n",
    "of the penalty parameter $\\lambda$ ($=1/C$).  Consider just the\n",
    "coefficients associated with estimation of the Matoke logit: If we try\n",
    "$P$ different values of the penalty parameter using $K$-fold\n",
    "cross-validation this will be $KP$ different estimates for every\n",
    "parameter.  We can average over the $K$ different folds to get a\n",
    "clearer picture of how coefficients vary with &lambda;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CVEsts['Matoke'].coefs_paths_[True].mean(axis=0),index=Lambdas.tolist(),columns=X.columns).T\n",
    "# select the CVEsts where coefs_paths_ == TRUE, average over the rows (axis=0)\n",
    "# name each row for a lambda, \n",
    "# name each column for a value in X\n",
    "# transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see also how the EMSE varies with $\\lambda$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMSEs={k:-e.scores_[True].mean(axis=0).ravel() for k,e in CVEsts.items()} \n",
    "# loop over CVEsts, make a new dictionary with the same keys, values are (- avg. value_k where scores_ = TRUE\n",
    "# (ravel casts this to an array)\n",
    "\n",
    "EMSEs = pd.DataFrame(EMSEs,index=np.log(Lambdas).tolist()).T\n",
    "# make this dictionary into a dataframe with index log(lamdas), transpose\n",
    "EMSEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these versus $\\log\\lambda$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMSEs.T.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the minima of these curves gives estimates of the optimal\n",
    "&lambda;:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_star = pd.Series({k:1/e.C_[0] for k,e in CVEsts.items()})\n",
    "lambda_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large values of &lambda; encourage parsimony in the selection of\n",
    "regressors, so it&rsquo;s not surprising to find that consumption items with\n",
    "large values of $\\lambda^*$  also have few regressors (this is the\n",
    "magic of Lasso):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_outcomes = pd.DataFrame({'#Regressors':(np.abs(CVCoefs)>1e-5).sum(),\n",
    "                               'λ*':lambda_star})\n",
    "Lasso_outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
