{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Estimation of Logit Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a &ldquo;logit&rdquo; regression; score function from MLE is:\n",
    "$$\n",
    "     \\frac{1}{N}\\sum_j X_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n",
    "  $$\n",
    "Let $u_j = y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}$; if we have some $Z$ such that\n",
    "$\\mbox{E}(u|Z) = 0$ then we can construct further moment conditions\n",
    "$$\n",
    "     \\frac{1}{N}\\sum_j Z_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Some different options for moments...\n",
    "\n",
    "def mle(b,y,X,Z):\n",
    "    \"\"\"Observations of score for MLE estimator.\n",
    "\n",
    "    Moment condition is E(y_j - p(x_jb))x_j = 0,\n",
    "    where p(xb) = e^{xb}/(1+e^{xb})\n",
    "    \"\"\"\n",
    "    p = np.exp(X*b)  # This is actually the odds\n",
    "    p = p/(1+p)      # This is probability y=1\n",
    "\n",
    "    return X*(y - p)\n",
    "\n",
    "def nonlinear_iv(b,y,X,Z):\n",
    "    \"\"\"Observations for restriction that Z\n",
    "    orthogonal to score.\n",
    "\n",
    "    Moment condition is E(Z_jy_j - Z_jexp(x_jb)) = 0\n",
    "    \"\"\"\n",
    "    p = np.exp(X*b)  # This is actually the odds\n",
    "    p = p/(1+p)      # This is probability y=1\n",
    "\n",
    "    return Z*(y - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generating Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "\n",
    "def dgp(N,beta,VXZ,gamma=1):\n",
    "    \"\"\"Generate a tuple of (y,X,Z).\n",
    "\n",
    "    Satisfies model:\n",
    "        Pr(y=1|X) = f(X@beta,gamma)\n",
    "        u = y - f(X@beta,gamma)\n",
    "        E(u|Z) = 0\n",
    "        f(x,gamma) = (exp(x)/(1+exp(x)))**gamma\n",
    "        Var([X,Z}) = VXZ\n",
    "        X,Z mean zero, Gaussian\n",
    "\n",
    "    Each element of the tuple is an array of N observations.\n",
    "    When gamma=1 this reduces to the logit model\n",
    "\n",
    "    Inputs include\n",
    "    - beta :: Governs effect of X on probability y=1\n",
    "    - gamma :: Governs curvature of function\n",
    "    - VXZ :: Var([X,Z])\n",
    "    \"\"\"\n",
    "    \n",
    "    # \"Square root\" of VXZ via eigendecomposition\n",
    "    lbda,v = np.linalg.eig(VXZ)\n",
    "    SXZ = v@np.diag(np.sqrt(lbda))\n",
    "\n",
    "    # Generate normal random variates [X*,Z]\n",
    "    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n",
    "\n",
    "    X = XZ[:,[0]] \n",
    "    Z = XZ[:,1:]\n",
    "\n",
    "    # Calculate y\n",
    "    pi = np.exp(X*beta)\n",
    "    pi = (pi/(1+pi))**gamma\n",
    "\n",
    "    y = iid.bernoulli(pi).rvs(size=(N,1))\n",
    "\n",
    "    return y,X,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Truth (Mark I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the truth the form of the distribution of $y$ is\n",
    "known up to the parameter $\\beta$; MLE takes advantage of this.\n",
    "\n",
    "Choose some parameters to establish the &ldquo;truth&rdquo;:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "## Play with us!\n",
    "beta = 2     # \"Coefficient of interest\"\n",
    "\n",
    "## Play with us!\n",
    "\n",
    "# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n",
    "\n",
    "ell = 1 # Play with me too!\n",
    "\n",
    "# Arbitrary (but deterministic) choice for VXZ\n",
    "A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n",
    "\n",
    "## Below here we're less playful.\n",
    "\n",
    "# Var([X,Z]|u) is constructed so that pos. def.\n",
    "VXZ = A.T@A \n",
    "\n",
    "truth = (beta,VXZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Analysis of MLE via GMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the score to be our moment condition; $Z$ doesn&rsquo;t appear,\n",
    "so estimator is just identified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmm # Just code defined last class, saved to .py file in this directory\n",
    "\n",
    "gmm.gj = mle  # Redefine gj function to use MLE score\n",
    "\n",
    "data = dgp(1000,*truth) \n",
    "# note the use of * to expand the elements of truth to the 2 remaining args of the f'n \n",
    "\n",
    "soltn = gmm.two_step_gmm(data)\n",
    "\n",
    "limiting_J = iid.chi2(0)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our experiment begins.  We set our frequentist hats firmly on our\n",
    "heads, and draw repeated samples of data, each generating a\n",
    "corresponding estimate of beta.  Then the empirical distribution of\n",
    "these samples tells us about the *finite* sample performance of our estimator.\n",
    "\n",
    "We&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\n",
    "samples of size $N$, until estimates of the covariance of our\n",
    "estimates converge:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmm # Just code defined last class\n",
    "\n",
    "N = 1000 # Sample size\n",
    "\n",
    "tol = 1e-3\n",
    "\n",
    "b_mle = []\n",
    "b_nliv = []\n",
    "J_nliv = []\n",
    "\n",
    "d=0\n",
    "oldV = 0\n",
    "newV = 1\n",
    "while d<30 or np.linalg.norm(oldV-newV) > tol:\n",
    "    # run 30 times or until the change in variance <= tolerance of .003\n",
    "    d += 1 # add 1 each time to d so that we don't get stuck in an infinite loop! \n",
    "    oldV = newV # save the value of newV from the last round\n",
    "    data = dgp(N,*truth)\n",
    "    gmm.gj = mle\n",
    "    soltn_mle = gmm.two_step_gmm(data)\n",
    "    b_mle.append(soltn_mle.x) # stick the solution onto the end of the b_mle array\n",
    "\n",
    "    gmm.gj = nonlinear_iv\n",
    "    soltn_nliv = gmm.two_step_gmm(data)\n",
    "    b_nliv.append(soltn_nliv.x)  # stick the solution onto the end of the b_nliv array\n",
    "    J_nliv.append(soltn_nliv.fun)# stick the solution onto the end of the J_nliv array\n",
    "\n",
    "    newV = np.var(b_nliv) \n",
    "    # save the new variance so that we can compare it to the last value, stored in oldV\n",
    "    \n",
    "    ## before starting the next round, the while loop will check that d<30 AND\n",
    "    ## the change between newV oldV is above tolerance. If one of these fails, the loop stops\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare MLE & NLIV estimates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_ = plt.scatter(b_mle,b_nliv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
